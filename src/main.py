# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TEfl_CMG1fjzkr4hWJEUOyhMrkR5pFX-
"""



# Step 5 (robust): build a single scoring template across models + quick subset
import os, pandas as pd, numpy as np

MODELS = ["llama3.2:latest", "mistral:7b", "deepseek-r1:8b"]

def sanitize_model_name(m):
    import re
    return re.sub(r'[^A-Za-z0-9_.-]+','_', m)

result_files = [f"./outputs/ollama_results_{sanitize_model_name(m)}.csv" for m in MODELS]

frames = []
for f in result_files:
    if os.path.exists(f):
        df_f = pd.read_csv(f)
        df_f["model"] = df_f.get("model", sanitize_model_name(os.path.basename(f).replace("ollama_results_","").replace(".csv","")))
        frames.append(df_f)
        print(f"Loaded {f}: {df_f.shape[0]} rows")
    else:
        print("Missing results file:", f)

assert frames, "No result files found."

merged = pd.concat(frames, ignore_index=True)

# Keep/rename expected columns
need = ["id","model","context","prompt","response"]
missing = [c for c in need if c not in merged.columns]
if missing:
    raise ValueError(f"Missing expected columns: {missing}")

# Coerce id -> int if possible, otherwise keep original
try:
    merged["id"] = pd.to_numeric(merged["id"], errors="raise").astype(int)
except Exception:
    merged["id"] = pd.to_numeric(merged["id"], errors="coerce")
    # If still NaN, give a warning but continue
    bad_ids = merged["id"].isna().sum()
    print(f"Warning: {bad_ids} rows have non-numeric id; keeping as NaN.")

# Deduplicate if any (e.g., from partial retries)
before = len(merged)
merged = merged.drop_duplicates(subset=["id","model"], keep="first")
after = len(merged)
if after < before:
    print(f"Deduplicated: removed {before-after} duplicate (id, model) rows")

# Sort nicely
merged = merged.sort_values(["model","id"]).reset_index(drop=True)

# Flag empty/errored responses for your awareness (kept in template so you can score as 0 if needed)
merged["_empty_resp"] = merged["response"].isna() | (merged["response"].astype(str).str.strip() == "")
merged["_error_resp"] = merged["response"].astype(str).str.startswith("[ERROR]")

for m in MODELS:
    sub = merged[merged["model"]==m]
    print(f"{m}: rows={len(sub)}, unique ids={sub['id'].nunique()}, empty={sub['_empty_resp'].sum()}, errors={sub['_error_resp'].sum()}")

# Build scoring template
score_df = merged[["id","model","context","prompt","response"]].copy()
for c in ["accuracy","cultural_relevance","language_accessibility","bias_avoidance"]:
    score_df[c] = ""

print("\n--- Scoring Template Preview ---")
print(score_df.head(6))
print("Total rows:", len(score_df))

scoring_csv = "./outputs/scoring_template_all_models.csv"
score_df.to_csv(scoring_csv, index=False, encoding="utf-8")
print("Saved scoring template →", scoring_csv)

# OPTIONAL: create a small subset for quick scoring (first 30 ids × all models)
subset_ids = sorted(merged["id"].dropna().unique())[:30]
subset_df = score_df[score_df["id"].isin(subset_ids)].copy()
subset_csv = "./outputs/scoring_subset_30x3.csv"
subset_df.to_csv(subset_csv, index=False, encoding="utf-8")
print("Saved quick subset for manual scoring →", subset_csv)

# OPTIONAL: print a tiny per-model preview
for m in MODELS:
    print(f"\n== {m} sample ==")
    print(score_df[score_df["model"]==m].head(2)[["id","context","response"]])

# Clean responses: strip <think>...</think> or bare <think> blocks, trim to ~180 words for consistency
import re, pandas as pd

scoring_csv = "./outputs/scoring_template_all_models.csv"
df = pd.read_csv(scoring_csv)

def strip_think(text: str) -> str:
    if not isinstance(text, str):
        return ""
    # remove <think>...</think> blocks or lone <think> tags
    text = re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL|re.IGNORECASE)
    text = re.sub(r"</?think>", "", text, flags=re.IGNORECASE)
    # collapse whitespace
    text = re.sub(r"\s+", " ", text).strip()
    return text

def hard_trim_words(text: str, max_words=180):
    words = text.split()
    return " ".join(words[:max_words])

df["clean_response"] = df["response"].apply(strip_think).apply(hard_trim_words)

# sanity: keep both original and clean in the CSV
df.to_csv(scoring_csv, index=False, encoding="utf-8")
print("Added clean_response to:", scoring_csv)
print(df[["id","model","clean_response"]].head(3))



# === Pre-evaluation diagnostics for 3 models =========================
# Creates a per-model table and 4 small bar charts you can include now.
# Input:  ./outputs/scoring_template_all_models.csv  (must include 'model', 'response', 'clean_response')
# Outputs: ./outputs/diagnostics_table.csv and ./outputs/plots_diagnostics/*.png

import os, re, math
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

IN = "./outputs/scoring_template_all_models.csv"
OUTDIR = "./outputs/plots_diagnostics"
os.makedirs(OUTDIR, exist_ok=True)

df = pd.read_csv(IN)

# --- helpers ---
def wc(text):
    if not isinstance(text, str): return 0
    return len(text.split())

def sent_count(text):
    if not isinstance(text, str): return 0
    # naive: split on . ! ? and keep non-empty segments
    parts = re.split(r"[.!?]+", text)
    return sum(1 for p in parts if p.strip())

def syllable_count(word):
    # extremely simple heuristic; good enough for relative FRE comparison
    w = word.lower()
    if not w: return 0
    # treat consecutive vowels as one syllable
    chunks = re.findall(r"[aeiouy]+", w)
    syl = len(chunks)
    # silent 'e'
    if w.endswith("e") and syl > 1:
        syl -= 1
    return max(1, syl)

def flesch_reading_ease(text):
    if not isinstance(text, str) or not text.strip():
        return np.nan
    words = re.findall(r"[A-Za-z']+", text)
    W = max(1, len(words))
    S = max(1, sent_count(text))
    syll = sum(syllable_count(w) for w in words)
    # FRE = 206.835 – 1.015*(W/S) – 84.6*(syll/W)
    return 206.835 - 1.015*(W/S) - 84.6*(syll/W)

# --- compute fields ---
df["raw_words"]   = df["response"].apply(wc)
df["clean_words"] = df["clean_response"].apply(wc)

MAXW = 180
df["trimmed_flag"] = df["clean_words"] >= MAXW

# structure & safety signals on CLEAN responses (what raters see)
def has_listy(t):
    if not isinstance(t, str): return False
    # bullets, numbered lists, dashes at line starts
    return bool(re.search(r"(^|\n)\s*(?:[-•*]|\d+\.)\s+", t))

SAFETY_PAT = re.compile(
    r"(seek (immediate )?medical|call\s*911|emergency department|talk to (a )?(doctor|clinician|nurse|provider)|"
    r"crisis (line|hotline)|mental health (professional|services))",
    re.IGNORECASE
)

def has_safety(t):
    if not isinstance(t, str): return False
    return bool(SAFETY_PAT.search(t))

# artifact prevalence BEFORE cleaning (just to document fairness)
def had_think(t):
    if not isinstance(t, str): return False
    return "<think" in t.lower()

df["list_struct"]   = df["clean_response"].apply(has_listy)
df["safety_msg"]    = df["clean_response"].apply(has_safety)
df["had_think_raw"] = df["response"].apply(had_think)

# --- aggregate per model ---
def pct(x):
    x = pd.Series(x)
    if len(x)==0: return np.nan
    return 100.0 * (x.sum()/len(x))

agg = df.groupby("model").apply(lambda g: pd.Series({
    "n": len(g),
    "mean_words": g["clean_words"].mean(),
    "median_words": g["clean_words"].median(),
    "iqr_words": np.subtract(*np.percentile(g["clean_words"].dropna(), [75,25])) if g["clean_words"].notna().any() else np.nan,
    "%_trimmed_180": pct(g["trimmed_flag"]),
    "FRE_mean": g["clean_response"].apply(flesch_reading_ease).mean(),
    "%_with_lists": pct(g["list_struct"]),
    "%_with_safety_msg": pct(g["safety_msg"]),
    "%_had_<think>_in_raw": pct(g["had_think_raw"]),
})).sort_index()

# round for display
disp = agg.copy()
for c in ["mean_words","median_words","iqr_words","%_trimmed_180","FRE_mean","%_with_lists","%_with_safety_msg","%_had_<think>_in_raw"]:
    disp[c] = disp[c].astype(float).round(2)

disp.to_csv("./outputs/diagnostics_table.csv")
print(disp)

# --- plotting helpers ---
def barplot(series, ylabel, title, fname, ylim=None):
    fig = plt.figure(figsize=(6,4.5))
    x = np.arange(len(series.index))
    plt.bar(x, series.values, capsize=0)
    plt.xticks(x, series.index, rotation=15)
    if ylim: plt.ylim(*ylim)
    plt.ylabel(ylabel)
    plt.title(title)
    plt.grid(axis="y", linestyle="--", alpha=0.4)
    plt.tight_layout()
    plt.savefig(os.path.join(OUTDIR, fname), dpi=200)
    plt.close(fig)

barplot(disp["mean_words"], "Words (mean)", "Mean clean response length", "bar_mean_words.png", ylim=(0, max(200, disp["mean_words"].max()+20)))
barplot(disp["%_trimmed_180"], "% responses trimmed (≥180 words)", "Verbosity vs. 180-word cap", "bar_pct_trimmed.png", ylim=(0,100))
barplot(disp["FRE_mean"], "Flesch Reading Ease (↑ easier)", "Readability (clean responses)", "bar_readability.png")
# combine list & safety into one figure by stacking? (Keep simple: plot separately or together)
# Here we create one chart with two bars per model by simple trick: not requested; keep one summary chart:
barplot(disp["%_with_lists"], "% responses with lists", "Structural signals: lists", "bar_struct_lists.png", ylim=(0,100))
barplot(disp["%_with_safety_msg"], "% responses with safety msg", "Structural signals: safety messaging", "bar_struct_safety.png", ylim=(0,100))
# Also document pre-clean artifact prevalence:
barplot(disp["%_had_<think>_in_raw"], "% with <think> artifacts (raw)", "Raw artifact prevalence", "bar_structural_signals.png", ylim=(0,100))



# Bar charts for model diagnostics (pure matplotlib, one chart per figure, no explicit colors).
# It will try to load ./outputs/diagnostics_table.csv (from the previous step).
# If that file doesn't exist, it recomputes diagnostics from ./outputs/scoring_template_all_models.csv.
# Figures are saved to /mnt/data/plots_diagnostics and also displayed inline.

import os, re, math
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

SAVE_DIR = "data/plots_diagnostics"
os.makedirs(SAVE_DIR, exist_ok=True)

diag_path = "./outputs/diagnostics_table.csv"
scoring_path = "./outputs/scoring_template_all_models.csv"

def wc(text):
    if not isinstance(text, str): return 0
    return len(text.split())

def sent_count(text):
    if not isinstance(text, str): return 0
    parts = re.split(r"[.!?]+", text)
    return sum(1 for p in parts if p.strip())

def syllable_count(word):
    w = word.lower()
    if not w: return 0
    chunks = re.findall(r"[aeiouy]+", w)
    syl = len(chunks)
    if w.endswith("e") and syl > 1:
        syl -= 1
    return max(1, syl)

def flesch_reading_ease(text):
    if not isinstance(text, str) or not text.strip():
        return np.nan
    words = re.findall(r"[A-Za-z']+", text)
    W = max(1, len(words))
    S = max(1, sent_count(text))
    syll = sum(syllable_count(w) for w in words)
    return 206.835 - 1.015*(W/S) - 84.6*(syll/W)

def has_listy(t):
    if not isinstance(t, str): return False
    return bool(re.search(r"(^|\n)\s*(?:[-•*]|\d+\.)\s+", t))

SAFETY_PAT = re.compile(
    r"(seek (immediate )?medical|call\s*911|emergency department|talk to (a )?(doctor|clinician|nurse|provider)|"
    r"crisis (line|hotline)|mental health (professional|services))",
    re.IGNORECASE
)

def has_safety(t):
    if not isinstance(t, str): return False
    return bool(SAFETY_PAT.search(t))

def had_think(t):
    if not isinstance(t, str): return False
    return "<think" in t.lower()

def pct(x):
    x = pd.Series(x)
    if len(x)==0: return np.nan
    return 100.0 * (x.sum()/len(x))

def compute_diagnostics_from_scoring(scoring_csv):
    df = pd.read_csv(scoring_csv)
    if "clean_response" not in df.columns:
        # if not present, fall back to using raw responses (not ideal, but allows plotting)
        df["clean_response"] = df["response"]

    df["raw_words"]   = df["response"].apply(wc)
    df["clean_words"] = df["clean_response"].apply(wc)

    MAXW = 180
    df["trimmed_flag"]   = df["clean_words"] >= MAXW
    df["list_struct"]    = df["clean_response"].apply(has_listy)
    df["safety_msg"]     = df["clean_response"].apply(has_safety)
    df["had_think_raw"]  = df["response"].apply(had_think)

    agg = df.groupby("model").apply(lambda g: pd.Series({
        "n": len(g),
        "mean_words": g["clean_words"].mean(),
        "median_words": g["clean_words"].median(),
        "iqr_words": np.subtract(*np.percentile(g["clean_words"].dropna(), [75,25])) if g["clean_words"].notna().any() else np.nan,
        "%_trimmed_180": pct(g["trimmed_flag"]),
        "FRE_mean": g["clean_response"].apply(flesch_reading_ease).mean(),
        "%_with_lists": pct(g["list_struct"]),
        "%_with_safety_msg": pct(g["safety_msg"]),
        "%_had_<think>_in_raw": pct(g["had_think_raw"]),
    })).sort_index()
    return agg

# Load existing diagnostics table or compute it
if os.path.exists(diag_path):
    diag = pd.read_csv(diag_path, index_col=0)
else:
    diag = compute_diagnostics_from_scoring(scoring_path)
    # Save a copy next to figures for reference
    diag_round = diag.copy()
    for c in ["mean_words","median_words","iqr_words","%_trimmed_180","FRE_mean","%_with_lists","%_with_safety_msg","%_had_<think>_in_raw"]:
        diag_round[c] = diag_round[c].astype(float).round(2)
    diag_round.to_csv(diag_path)

# Ensure numeric types
for c in ["mean_words","%_trimmed_180","FRE_mean","%_with_safety_msg","%_had_<think>_in_raw"]:
    diag[c] = pd.to_numeric(diag[c], errors="coerce")

# Simple bar plotting function
def barplot(series, ylabel, title, filename, ylim=None):
    fig = plt.figure(figsize=(6,4.5))
    x = np.arange(len(series.index))
    plt.bar(x, series.values)
    plt.xticks(x, series.index, rotation=15)
    if ylim: plt.ylim(*ylim)
    plt.ylabel(ylabel)
    plt.title(title)
    plt.grid(axis="y", linestyle="--", alpha=0.4)
    plt.tight_layout()
    path = os.path.join(SAVE_DIR, filename)
    plt.savefig(path, dpi=200)
    plt.show()  # display inline
    plt.close(fig)
    return path

paths = {}
paths["mean_words"] = barplot(
    diag["mean_words"],
    "Words (mean)",
    "Mean Clean Response Length",
    "bar_mean_words.png",
    ylim=(0, max(200, float(diag["mean_words"].max()) + 20))
)
paths["pct_trimmed"] = barplot(
    diag["%_trimmed_180"],
    "% Responses Trimmed (≥180 words)",
    "Verbosity vs. 180-word Cap",
    "bar_pct_trimmed.png",
    ylim=(0, 100)
)
paths["readability"] = barplot(
    diag["FRE_mean"],
    "Flesch Reading Ease (↑ easier)",
    "Readability of Clean Responses",
    "bar_readability.png"
)
paths["safety"] = barplot(
    diag["%_with_safety_msg"],
    "% with Explicit Safety Messaging",
    "Safety Messaging by Model",
    "bar_safety.png",
    ylim=(0, 100)
)
paths["think"] = barplot(
    diag["%_had_<think>_in_raw"],
    "% with <think> Artifacts (Raw Output)",
    "Artifact Prevalence Before Cleaning",
    "bar_think_artifacts.png",
    ylim=(0, 100)
)

paths



# TEST ONLY: fill first 10 rows in the main scoring file (reproducible)
import numpy as np, pandas as pd
np.random.seed(42)

scoring_csv = "./outputs/scoring_template_all_models.csv"
df_fill = pd.read_csv(scoring_csv)
metrics = ["accuracy","cultural_relevance","language_accessibility","bias_avoidance"]

df_fill.loc[:9, metrics] = np.random.randint(0, 4, size=(10, len(metrics)))
df_fill.to_csv(scoring_csv, index=False, encoding="utf-8")

print("Filled first 10 rows with random scores for testing.")
print(df_fill.head(12)[["id","model"]+metrics])



# TEST ONLY: make a dev copy and fill the first 10 rows in that copy
import numpy as np, pandas as pd, shutil
np.random.seed(42)

src = "./outputs/scoring_template_all_models.csv"
dst = "./outputs/scoring_template_all_models_DEV.csv"
shutil.copyfile(src, dst)

df_fill = pd.read_csv(dst)
metrics = ["accuracy","cultural_relevance","language_accessibility","bias_avoidance"]
df_fill.loc[:9, metrics] = np.random.randint(0, 4, size=(10, len(metrics)))
df_fill.to_csv(dst, index=False, encoding="utf-8")

print("Created DEV scoring file and filled first 10 rows:", dst)
print(df_fill.head(12)[["id","model"]+metrics])

# auto sanity metrics: response length + presence of FNMWCF keywords
import pandas as pd, numpy as np, re

scoring_csv = "./outputs/scoring_template_all_models.csv"
s = pd.read_csv(scoring_csv)

# choose clean_response if present
resp_col = "clean_response" if "clean_response" in s.columns else "response"

s["resp_len"] = s[resp_col].astype(str).str.split().apply(len)

keywords = [
    r"\bculture\b", r"\bElders?\b", r"\bland-based\b", r"\btelehealth\b",
    r"\bcrisis\b", r"\baftercare\b", r"\btrauma-informed\b",
    r"\bhope\b", r"\bbelonging\b", r"\bmeaning\b", r"\bpurpose\b"
]
pat = re.compile("|".join(keywords), flags=re.IGNORECASE)
s["fnmwcf_keyword_hit"] = s[resp_col].astype(str).str.contains(pat)

print("\nPer-model quick stats:")
print(s.groupby("model")["resp_len"].agg(["mean","median","min","max","std"]).round(1))
print("\nFNMWCF keyword hit-rate by model:")
print(s.groupby("model")["fnmwcf_keyword_hit"].mean().round(3))



# # FNMWCF theme coverage with prompt-leakage control

# FNMWCF theme coverage with prompt-leakage control
import re, os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

IN = "./outputs/scoring_template_all_models.csv"
OUT = "./outputs/plots_fnmwcf"
os.makedirs(OUT, exist_ok=True)

df = pd.read_csv(IN)
resp_col = "clean_response" if "clean_response" in df.columns else "response"

# --- define FNMWCF themes (make terms specific; avoid ultra-common ones) ---
THEMES = {
    "Culture/Elders/Land": [
        r"\bElders?\b", r"\bknowledge[-\s]?keepers?\b", r"\bland[-\s]?based\b",
        r"\bcultural (?:supports?|activities|safety)\b", r"\bsmudg(?:e|ing)\b",
        r"\bTwo[-\s]?Spirit\b"
    ],
    "Services/Access": [
        r"\btelehealth\b", r"\bcrisis (?:line|services|support)\b",
        r"\baftercare\b", r"\bprimary care\b", r"\bcommunity health (?:centre|center|nurse)\b"
    ],
    "Trauma-informed": [
        r"\btrauma[-\s]?informed\b", r"\bhistorical trauma\b", r"\bintergenerational trauma\b",
        r"\bharm[-\s]?reduction\b", r"\bstrength[-\s]?based\b"
    ],
    "Purpose/Meaning/Belonging": [
        r"\bbelonging\b", r"\bconnection to (?:community|culture|land)\b",
        r"\bpurpose\b", r"\bmeaning\b", r"\bidentity\b"
    ],
}

def compile_or(patterns):
    return re.compile("|".join(patterns), re.IGNORECASE)

THEME_PAT = {k: compile_or(v) for k, v in THEMES.items()}

# --- control for prompt leakage: remove keywords already in the prompt text ---
def theme_hits_excluding_prompt(resp, prompt):
    hits = {}
    ptxt = prompt if isinstance(prompt, str) else ""
    rtxt = resp if isinstance(resp, str) else ""
    for theme, pat in THEME_PAT.items():
        in_resp = bool(pat.search(rtxt))
        in_prompt = bool(pat.search(ptxt))
        hits[theme] = (in_resp and not in_prompt)  # count only if new in response
    return hits

theme_rows = []
for _, row in df.iterrows():
    hits = theme_hits_excluding_prompt(row.get(resp_col, ""), row.get("prompt", ""))
    hits["model"] = row.get("model", "")
    hits["themes_covered"] = sum(bool(v) for v in hits.values())
    theme_rows.append(hits)

tdf = pd.DataFrame(theme_rows)

# --- per-model coverage: percent of responses covering each theme ---
per_theme = tdf.groupby("model")[[t for t in THEMES.keys()]].mean().mul(100).round(1)
# --- distribution of themes_covered (0..4) per model ---
dist = tdf.groupby(["model","themes_covered"]).size().groupby(level=0).apply(lambda s: (s/s.sum())*100).unstack(fill_value=0).round(1)

print("Per-theme coverage (% of responses mentioning the theme, excluding prompt leakage):")
print(per_theme)
print("\nDistribution of number of FNMWCF themes covered per response (%):")
print(dist)

# --- plot per-theme coverage (one chart per theme) ---
def barplot(df_series, ylabel, title, fname, ylim=(0,100)):
    fig = plt.figure(figsize=(6,4.5))
    x = np.arange(len(df_series.index))
    plt.bar(x, df_series.values)
    plt.xticks(x, df_series.index, rotation=15)
    if ylim: plt.ylim(*ylim)
    plt.ylabel(ylabel); plt.title(title)
    plt.grid(axis="y", linestyle="--", alpha=.4)
    plt.tight_layout(); plt.savefig(os.path.join(OUT, fname), dpi=200); plt.close(fig)

for theme in per_theme.columns:
    barplot(per_theme[theme], "% responses", f"FNMWCF: {theme}", f"fnmwcf_{theme.replace('/','_')}.png")

# --- plot distribution of themes_covered ---
for k in sorted(dist.columns):
    barplot(dist[k], "% of responses", f"Responses covering {k} themes", f"fnmwcf_n{k}.png")





# Step 6: quick filler for testing visuals (overwrites ONLY first 10 rows)
import numpy as np

scoring_csv = "./outputs/scoring_template_all_models.csv"
df_fill = pd.read_csv(scoring_csv)
metrics = ["accuracy","cultural_relevance","language_accessibility","bias_avoidance"]
df_fill.loc[:9, metrics] = np.random.randint(0, 4, size=(10, len(metrics)))
df_fill.to_csv(scoring_csv, index=False, encoding="utf-8")

print("Filled first 10 rows with random scores for testing.")
print(df_fill.head(12)[["id","model"]+metrics])

# OPTIONAL: create a small subset for quick scoring (first 30 ids × all models)
subset_ids = sorted(merged["id"].dropna().unique())[:30]
subset_df = score_df[score_df["id"].isin(subset_ids)].copy()
subset_csv = "./outputs/scoring_subset_30x3.csv"
subset_df.to_csv(subset_csv, index=False, encoding="utf-8")
print("Saved quick subset for manual scoring →", subset_csv)

# OPTIONAL: print a tiny per-model preview
for m in MODELS:
    print(f"\n== {m} sample ==")
    print(score_df[score_df["model"]==m].head(2)[["id","context","response"]])

# STEP 7 — Aggregate results, stats, and plots (paper-ready)
import os, re, math
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import kruskal, mannwhitneyu

# --------- 1) Load scored data (prefer DEV) ----------
PATH_MAIN = "./outputs/scoring_template_all_models.csv"
PATH_DEV  = "./outputs/scoring_template_all_models_DEV.csv"
scoring_csv = PATH_DEV if os.path.exists(PATH_DEV) else PATH_MAIN
print("Using scoring file:", scoring_csv)

df = pd.read_csv(scoring_csv)

# Ensure expected columns exist
metrics = ["accuracy","cultural_relevance","language_accessibility","bias_avoidance"]
need = ["id","model","context","prompt","response"] + metrics
missing = [c for c in need if c not in df.columns]
if missing:
    raise ValueError(f"Missing columns in scoring file: {missing}")

# Use clean_response if present
resp_col = "clean_response" if "clean_response" in df.columns else "response"

# Coerce metrics to numeric; compute total
for m in metrics:
    df[m] = pd.to_numeric(df[m], errors="coerce")
df["total_score"] = df[metrics].sum(axis=1, min_count=1)

# Keep only rows with at least one scored metric
scored = df.dropna(subset=metrics, how="all").copy()
print(f"Scored rows: {len(scored)} / {len(df)}")

# --------- 2) Per-model summaries ----------
# --------- 2) Per-model summaries ----------
def sem(x):
    x = pd.Series(x).dropna().astype(float)
    return x.std(ddof=1) / math.sqrt(len(x)) if len(x) > 1 else np.nan

agg_mean = scored.groupby("model")[metrics + ["total_score"]].mean().round(3)

# compute SEM per metric per model
agg_sem = scored.groupby("model")[metrics + ["total_score"]].agg(sem).round(3)

os.makedirs("./outputs/tables", exist_ok=True)
agg_mean.to_csv("./outputs/tables/means_by_model.csv")
agg_sem.to_csv("./outputs/tables/sems_by_model.csv")
print("Saved tables: means_by_model.csv, sems_by_model.csv")

print("\nMeans by model:\n", agg_mean)
print("\nSEMs by model:\n", agg_sem)

# --------- 3) Nonparametric significance tests ----------
## ---------------- 3) Nonparametric significance tests (robust guards) ----------------
models_all = sorted(scored["model"].unique())

# models that actually have >=2 scored rows (any metric filled)
def n_scored_rows(d):
    # count rows with at least one metric filled
    return d[metrics].notna().any(axis=1).sum()

models_ok = []
for mdl in models_all:
    n = n_scored_rows(scored[scored["model"]==mdl])
    print(f"{mdl}: scored rows = {n}")
    if n >= 2:
        models_ok.append(mdl)

if len(models_ok) < 2:
    print("Skipping Kruskal–Wallis and pairwise tests: need ≥2 models with ≥2 scored rows each.")
    kw_df = pd.DataFrame(columns=["metric","kruskal_H","p_value"])
    pair_df = pd.DataFrame(columns=["metric","model_a","model_b","U","p_raw","p_bonf"])
else:
    kw_results = []
    for mtr in metrics+["total_score"]:
        groups = [scored.loc[scored["model"]==mdl, mtr].dropna() for mdl in models_ok]
        if all(len(g)>1 for g in groups):
            stat, p = kruskal(*groups)
            kw_results.append({"metric": mtr, "kruskal_H": stat, "p_value": p})
    kw_df = pd.DataFrame(kw_results)
    kw_df.to_csv("./outputs/tables/kruskal_results.csv", index=False)
    print("\nKruskal–Wallis results:\n", kw_df)

    # Pairwise Mann–Whitney + Bonferroni
    pairs = []
    for mtr in metrics+["total_score"]:
        for i in range(len(models_ok)):
            for j in range(i+1, len(models_ok)):
                a = scored.loc[scored["model"]==models_ok[i], mtr].dropna()
                b = scored.loc[scored["model"]==models_ok[j], mtr].dropna()
                if len(a)>1 and len(b)>1:
                    U, p = mannwhitneyu(a, b, alternative="two-sided")
                    pairs.append({"metric": mtr, "model_a": models_ok[i], "model_b": models_ok[j], "U": U, "p_raw": p})
    pair_df = pd.DataFrame(pairs)
    if not pair_df.empty:
        pair_df["p_bonf"] = (pair_df["p_raw"] * len(pair_df)).clip(upper=1.0)
        pair_df.to_csv("./outputs/tables/pairwise_mannwhitney_bonf.csv", index=False)
        print("\nPairwise Mann–Whitney (Bonferroni):\n", pair_df.head())

# --------- 4) FNMWCF keyword coverage ----------
keywords = [
    r"\bculture\b", r"\bElders?\b", r"\bland[- ]based\b", r"\btelehealth\b",
    r"\bcrisis\b", r"\baftercare\b", r"\btrauma[- ]informed\b",
    r"\bhope\b", r"\bbelonging\b", r"\bmeaning\b", r"\bpurpose\b"
]
pat = re.compile("|".join(keywords), flags=re.IGNORECASE)

def count_hits(text: str) -> int:
    if not isinstance(text, str): return 0
    return len(re.findall(pat, text))

df["fnmwcf_hits"] = df[resp_col].astype(str).apply(count_hits)
hits_mean = df.groupby("model")["fnmwcf_hits"].mean().round(3)
hits_mean.to_csv("./outputs/tables/fnmwcf_hits_mean.csv")
print("\nFNMWCF keyword hits (mean per response):\n", hits_mean)

# --------- 5) Response length ----------
df["resp_len"] = df[resp_col].astype(str).str.split().apply(len)
len_stats = df.groupby("model")["resp_len"].agg(["mean","median","min","max","std"]).round(1)
len_stats.to_csv("./outputs/tables/response_length_stats.csv")
print("\nResponse length stats:\n", len_stats)

# --------- 6) Plots (matplotlib, one chart per figure, no colors) ----------
os.makedirs("./outputs/plots", exist_ok=True)

# (a) Mean score by criterion per model (separate figure per metric)
for mtr in metrics+["total_score"]:
    plt.figure()
    means = scored.groupby("model")[mtr].mean()
    errs  = scored.groupby("model")[mtr].apply(sem)
    means.plot(kind="bar", yerr=errs, capsize=4)
    plt.title(f"Mean {mtr} by model")
    plt.xlabel("Model"); plt.ylabel(f"{mtr} (0–3 for criteria, 0–12 total)")
    plt.tight_layout()
    outp = f"./outputs/plots/mean_{mtr}_by_model.png"
    plt.savefig(outp)
    plt.close()
    print("Saved", outp)

# (b) Boxplot of total_score by model
plt.figure()
data = [scored.loc[scored["model"]==mdl, "total_score"].dropna() for mdl in models]
plt.boxplot(data, labels=models, showfliers=False)
plt.title("Total score distribution by model")
plt.xlabel("Model"); plt.ylabel("Total score (0–12)")
plt.tight_layout()
plt.savefig("./outputs/plots/box_total_score_by_model.png")
plt.close()
print("Saved ./outputs/plots/box_total_score_by_model.png")

# (c) FNMWCF keyword hits — mean bar chart
plt.figure()
hits_mean.plot(kind="bar")
plt.title("FNMWCF keyword hits (mean per response)")
plt.xlabel("Model"); plt.ylabel("Mean hits per response")
plt.tight_layout()
plt.savefig("./outputs/plots/fnmwcf_hits_mean.png")
plt.close()
print("Saved ./outputs/plots/fnmwcf_hits_mean.png")

# (d) Response length boxplot by model
plt.figure()
ldata = [df.loc[df["model"]==mdl, "resp_len"].dropna() for mdl in models]
plt.boxplot(ldata, labels=models, showfliers=False)
plt.title("Response length by model")
plt.xlabel("Model"); plt.ylabel("Word count")
plt.tight_layout()
plt.savefig("./outputs/plots/box_response_length_by_model.png")
plt.close()
print("Saved ./outputs/plots/box_response_length_by_model.png")

# --------- 7) Qualitative review set (lowest-scoring examples) ----------
def lowest_examples(d, metric="total_score", k=5):
    sub = d.dropna(subset=[metric]).sort_values(metric, ascending=True)
    return sub.head(k)[["id","model","context",resp_col,metric]]

qual_rows = []
for mdl in models:
    q = lowest_examples(scored[scored["model"]==mdl], k=5)
    qual_rows.append(q)
qual = pd.concat(qual_rows, ignore_index=True)
qual_out = "./outputs/tables/qualitative_lowest_examples.csv"
qual.to_csv(qual_out, index=False)
print("Saved qualitative sample →", qual_out)
print("\nDone.")

import re, os
import pandas as pd
from pathlib import Path

def parse_ctx(s):
    d = {}
    if not isinstance(s, str): return d
    # expect patterns like: province=SK; sex=Female; age_group=25–34; ...
    for part in s.split(";"):
        kv = part.strip().split("=", 1)
        if len(kv)==2:
            k, v = kv[0].strip(), kv[1].strip()
            d[k] = v
    return d

result_files = [
    "./outputs/ollama_results_llama3.2_latest.csv",
    "./outputs/ollama_results_mistral_7b.csv",
    "./outputs/ollama_results_deepseek-r1_8b.csv",
]
dfs = [pd.read_csv(f) for f in result_files if os.path.exists(f)]
merged = pd.concat(dfs, ignore_index=True)

ctx_parsed = merged["context"].apply(parse_ctx).apply(pd.Series)
merged_ctx = pd.concat([merged, ctx_parsed], axis=1)

# Keep a few common strata
for col in ["province","sex","age_group"]:
    if col not in merged_ctx.columns:
        merged_ctx[col] = "UNK"

# Sample ~10 per model across strata
want_per_model = 10
balanced = []
for m, sub in merged_ctx.groupby("model"):
    # prioritize diversity by province/sex/age_group
    # take up to 2 per unique (province, sex) first
    picked_idx = []
    for (prov, sex), g in sub.groupby(["province","sex"]):
        take = g.sample(min(2, len(g)), random_state=42).index.tolist()
        picked_idx.extend(take)
        if len(picked_idx) >= want_per_model:
            break
    # top up if needed
    if len(picked_idx) < want_per_model:
        remaining = sub.drop(index=picked_idx)
        extra = remaining.sample(min(want_per_model - len(picked_idx), len(remaining)), random_state=123).index.tolist()
        picked_idx.extend(extra)
    balanced.append(sub.loc[picked_idx])

balanced_df = pd.concat(balanced, ignore_index=True)

# Keep scoring fields
keep_cols = ["id","model","context","prompt","response"]
score_cols = ["accuracy","cultural_relevance","language_accessibility","bias_avoidance"]
out = balanced_df[keep_cols].copy()
for c in score_cols: out[c] = ""

# Prefer clean_response if you already computed it
if "clean_response" in merged.columns:
    out = out.merge(merged[["id","model","clean_response"]], on=["id","model"], how="left")

Path("./outputs").mkdir(parents=True, exist_ok=True)
subset_path = "./outputs/scoring_subset_balanced_10_per_model.csv"
out.to_csv(subset_path, index=False, encoding="utf-8")
print("Wrote balanced subset to:", subset_path)
print(out.head(3))

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

subset_path = "./outputs/scoring_subset_balanced_10_per_model.csv"
df = pd.read_csv(subset_path)

# Parse province/sex/age from the 'context' field (if not already split)
def parse_ctx(s):
    d = {}
    if isinstance(s, str):
        for part in s.split(";"):
            kv = part.strip().split("=", 1)
            if len(kv)==2:
                d[kv[0].strip()] = kv[1].strip()
    return pd.Series({"province": d.get("province","UNK"),
                      "sex": d.get("sex","UNK"),
                      "age_group": d.get("age_group","UNK")})

ctx = df["context"].apply(parse_ctx)
d = pd.concat([df[["model"]], ctx], axis=1)

# 1) Small coverage table (province × sex) collapsed across age
cov = d.groupby(["model","province","sex"]).size().reset_index(name="n").sort_values(["model","province","sex"])
cov_pivot = cov.pivot_table(index=["province","sex"], columns="model", values="n", fill_value=0)
cov_pivot



cov_pivot.to_csv("./outputs/tables/pilot_subset_coverage_province_sex.csv")

for mdl, g in d.groupby("model"):
    counts = g["province"].value_counts().sort_index()
    plt.figure(figsize=(6,4.5))
    counts.plot(kind="bar")
    plt.title(f"Pilot subset coverage by province — {mdl}")
    plt.ylabel("Count"); plt.xlabel("Province")
    plt.grid(axis="y", linestyle="--", alpha=0.4)
    plt.tight_layout()
    plt.savefig(f"./outputs/plots/pilot_subset_province_counts__{mdl}.png", dpi=200)
    plt.close()

# ===== Rich coverage tables + charts (no seaborn, one chart per figure) =====
import os, re
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

SUBSET_PATH = "./outputs/scoring_subset_balanced_10_per_model.csv"
TABLE_DIR   = "./outputs/tables"
PLOT_DIR    = "./outputs/plots"
os.makedirs(TABLE_DIR, exist_ok=True)
os.makedirs(PLOT_DIR, exist_ok=True)

# -- Parse helper (province, sex, age_group) from 'context'
def parse_ctx(s):
    d = {}
    if isinstance(s, str):
        for part in s.split(";"):
            k,v = [t.strip() for t in part.split("=",1)] if "=" in part else ("","")
            if k: d[k]=v
    return pd.Series({
        "province": d.get("province","UNK"),
        "sex": d.get("sex","UNK"),
        "age_group": d.get("age_group","UNK"),
    })

df = pd.read_csv(SUBSET_PATH)
ctx = df["context"].apply(parse_ctx)
d = pd.concat([df[["model"]], ctx], axis=1)

# ---------- 1) PROVINCE × SEX × AGE (full detail) ----------
cov_full = (
    d.groupby(["province","sex","age_group","model"])
     .size().reset_index(name="n")
     .sort_values(["province","sex","age_group","model"])
)

# Pivot with models as columns; add row totals and per-model percentages
cov_full_p = cov_full.pivot_table(index=["province","sex","age_group"], columns="model", values="n", fill_value=0)

# Row totals
cov_full_p["Row_Total"] = cov_full_p.sum(axis=1)

# Per-model totals for percent calc
model_totals = cov_full_p.drop(columns=["Row_Total"]).sum(axis=0)
cov_full_pct = cov_full_p.drop(columns=["Row_Total"]).div(model_totals, axis=1) * 100.0
cov_full_pct = cov_full_pct.round(1)
cov_full_pct["Row_Total_%"] = cov_full_pct.sum(axis=1)  # not 100; sums across models

cov_full_p.to_csv(f"{TABLE_DIR}/pilot_subset_coverage__prov_sex_age_counts.csv")
cov_full_pct.to_csv(f"{TABLE_DIR}/pilot_subset_coverage__prov_sex_age_percent_by_model.csv")

print("Saved:",
      f"{TABLE_DIR}/pilot_subset_coverage__prov_sex_age_counts.csv",
      f"{TABLE_DIR}/pilot_subset_coverage__prov_sex_age_percent_by_model.csv",
      sep="\n  ")

# ---------- 2) PROVINCE × SEX (collapsed across age) ----------
cov_ps = (
    d.groupby(["province","sex","model"])
     .size().reset_index(name="n")
     .sort_values(["province","sex","model"])
)
cov_ps_p = cov_ps.pivot_table(index=["province","sex"], columns="model", values="n", fill_value=0)
cov_ps_p["Row_Total"] = cov_ps_p.sum(axis=1)
cov_ps_p.to_csv(f"{TABLE_DIR}/pilot_subset_coverage__prov_sex_counts.csv")
print(f"\nSaved: {TABLE_DIR}/pilot_subset_coverage__prov_sex_counts.csv")

# ---------- 3) GROUPED BAR: counts by (province × sex) across models ----------
# Build a tidy frame for plotting
plot_df = cov_ps.copy()
labels = plot_df[["province","sex"]].astype(str).agg(" · ".join, axis=1)
x_groups = labels.unique().tolist()
models = plot_df["model"].unique().tolist()

# Create a matrix counts[model, x]
counts = np.zeros((len(models), len(x_groups)), dtype=float)
for i, m in enumerate(models):
    sub = plot_df[plot_df["model"]==m]
    for j, lab in enumerate(x_groups):
        val = sub[(sub["province"]+" · "+sub["sex"])==lab]["n"]
        counts[i, j] = float(val.iloc[0]) if not val.empty else 0.0

# Plot grouped bars
fig = plt.figure(figsize=(10, 4.8))
x = np.arange(len(x_groups))
w = 0.8 / max(1, len(models))
for i, m in enumerate(models):
    plt.bar(x + i*w - (len(models)-1)*w/2, counts[i], width=w, label=m)
plt.xticks(x, x_groups, rotation=20)
plt.ylabel("Count")
plt.title("Pilot subset coverage by province × sex (counts)")
plt.grid(axis="y", linestyle="--", alpha=0.4)
plt.legend(title="Model")
plt.tight_layout()
gb_path = f"{PLOT_DIR}/pilot_subset_grouped_counts__province_sex.png"
plt.savefig(gb_path, dpi=200)
plt.show()
print("Saved:", gb_path)

# ---------- 4) 100% STACKED BAR: per-model distribution across provinces ----------
prov_counts = d.groupby(["model","province"]).size().unstack(fill_value=0)
prov_pct = prov_counts.div(prov_counts.sum(axis=1), axis=0) * 100.0

fig = plt.figure(figsize=(8, 4.8))
x = np.arange(len(prov_pct.index))
bottom = np.zeros(len(prov_pct.index))
for col in prov_pct.columns:
    plt.bar(x, prov_pct[col].values, bottom=bottom, label=col)
    bottom += prov_pct[col].values
plt.xticks(x, prov_pct.index, rotation=15)
plt.ylabel("% of pilot subset")
plt.title("Per-model province distribution (100% stacked)")
plt.grid(axis="y", linestyle="--", alpha=0.4)
plt.legend(title="Province", ncol=min(3, len(prov_pct.columns)))
plt.tight_layout()
stk_path = f"{PLOT_DIR}/pilot_subset_stacked100__province_distribution.png"
plt.savefig(stk_path, dpi=200)
plt.show()
print("Saved:", stk_path)

import pandas as pd

# Show province × sex × age counts
counts_df = pd.read_csv("./outputs/tables/pilot_subset_coverage__prov_sex_age_counts.csv")
print("Counts (province × sex × age):")
display(counts_df)

# Show province × sex × age percentages by model
pct_df = pd.read_csv("./outputs/tables/pilot_subset_coverage__prov_sex_age_percent_by_model.csv")
print("\nPercent coverage (province × sex × age, by model):")
display(pct_df)

# Show collapsed province × sex counts
prov_sex_df = pd.read_csv("./outputs/tables/pilot_subset_coverage__prov_sex_counts.csv")
print("\nCounts (province × sex, collapsed across age):")
display(prov_sex_df)

import pandas as pd

merged_path = "./outputs/scoring_template_all_models.csv"
subset_path = "./outputs/scoring_subset_balanced_10_per_model.csv"

def parse_ctx(s):
    d={}
    if isinstance(s,str):
        for part in s.split(";"):
            if "=" in part:
                k,v = [t.strip() for t in part.split("=",1)]
                d[k]=v
    return pd.Series({"province": d.get("province","UNK"),
                      "sex": d.get("sex","UNK"),
                      "age_group": d.get("age_group","UNK")})

# What's in the full 1350?
full_df = pd.read_csv(merged_path)
full_ctx = full_df["context"].apply(parse_ctx)
print("FULL — unique provinces:", sorted(full_ctx["province"].unique()))
print(full_ctx.groupby(["province","sex"]).size().sort_index())

# What's in the 30-row pilot subset?
sub_df = pd.read_csv(subset_path)
sub_ctx = sub_df["context"].apply(parse_ctx)
print("\nSUBSET — unique provinces:", sorted(sub_ctx["province"].unique()))
print(sub_ctx.groupby(["province","sex"]).size().sort_index())

import pandas as pd
from pathlib import Path

DEV = Path("./outputs/scoring_template_all_models_DEV.csv")
SUB = Path("./outputs/scoring_subset_balanced_10_per_model.csv")

df_dev = pd.read_csv(DEV)
df_sub = pd.read_csv(SUB)

metrics = ["accuracy","cultural_relevance","language_accessibility","bias_avoidance"]

# Merge on (id, model) and prefer non-null scores from subset
merged = df_dev.merge(df_sub[["id","model"]+metrics], on=["id","model"], how="left", suffixes=("","_new"))

for m in metrics:
    merged[m] = merged[m].combine_first(merged[m + "_new"])
    merged.drop(columns=[m + "_new"], inplace=True)

merged.to_csv(DEV, index=False, encoding="utf-8")
print("Updated:", DEV)

import pandas as pd
from pathlib import Path

SCORING = Path("./outputs/scoring_template_all_models_DEV.csv")
df = pd.read_csv(SCORING)
metrics = ["accuracy","cultural_relevance","language_accessibility","bias_avoidance"]
resp_col = "clean_response" if "clean_response" in df.columns else "response"

def next_unscored(mdl):
    sub = df[(df["model"]==mdl) & (df[metrics].isna().all(axis=1))]
    return sub.iloc[0] if len(sub) else None

for mdl in sorted(df["model"].unique()):
    for _ in range(3):  # score 3 items per model
        r = next_unscored(mdl)
        if r is None: break
        print("\n---", mdl, "ID:", int(r["id"]), "---")
        print("Context:", r["context"])
        print("Response:\n", r[resp_col][:700], "...")
        # ENTER your rubric scores here (0/1/2/3)
        df.loc[r.name, metrics] = [2, 2, 3, 2]  # <— edit these as you score

df.to_csv(SCORING, index=False, encoding="utf-8")
print("Saved:", SCORING)

# Safe rater logging: writes to a separate file, keeps audit trail, avoids overwrites.
import pandas as pd, numpy as np, time
from pathlib import Path

RATER_ID = "raterA"  # <-- set your initials
SRC = Path("./outputs/scoring_template_all_models.csv")       # main template
OUT = Path(f"./outputs/scoring_logs/{RATER_ID}_scores.csv")   # per-rater log
OUT.parent.mkdir(parents=True, exist_ok=True)

metrics = ["accuracy","cultural_relevance","language_accessibility","bias_avoidance"]
df = pd.read_csv(SRC)
resp_col = "clean_response" if "clean_response" in df.columns else "response"

# pick 3 unscored per model (example); change N as needed
N = 3
def pick_unscored(mdl):
    sub = df[(df["model"]==mdl) & (df[metrics].isna().all(axis=1))]
    return sub.sample(min(N, len(sub)), random_state=42)

batch = pd.concat([pick_unscored(m) for m in sorted(df["model"].unique())], ignore_index=True)

# Create a blank log (append-only)
cols = ["id","model","timestamp","rater_id"] + metrics
log = pd.DataFrame(columns=cols)

# --- Example scoring: replace with your real scores before running ---
# Here we just assign a placeholder vector so the code runs; DO NOT use for analysis.
placeholder = [2,2,3,2]
for _, r in batch.iterrows():
    log.loc[len(log)] = [int(r["id"]), r["model"], time.strftime("%Y-%m-%d %H:%M:%S"),
                         RATER_ID] + placeholder

# Merge with any existing log, drop duplicates (id,model,rater_id)
if OUT.exists():
    old = pd.read_csv(OUT)
    log = pd.concat([old, log], ignore_index=True)
log = log.drop_duplicates(subset=["id","model","rater_id"], keep="first")
log.to_csv(OUT, index=False)
print("Wrote scores to:", OUT)

import pandas as pd

log = pd.read_csv("outputs/scoring_logs/raterA_scores.csv")
print(log.head())        # show first 5 rows
print(log.shape)         # see how many rows/cols

from tabulate import tabulate
import pandas as pd

log = pd.read_csv("outputs/scoring_logs/raterA_scores.csv")
print(tabulate(log.head(10), headers="keys", tablefmt="psql"))

import pandas as pd

log = pd.read_csv("outputs/scoring_logs/raterA_scores.csv")
pd.set_option("display.colheader_justify", "center")  # center headers
log.head(10).style.set_table_styles(
    [{"selector":"th", "props":[("font-size","12pt"),("text-align","center")]}]
).background_gradient(cmap="Blues")

summary = log.groupby("model")[["accuracy","cultural_relevance",
                                "language_accessibility","bias_avoidance"]].mean().round(2)
print(summary)

a = summary.to_csv("outputs/scoring_logs/raterA_summary.csv")

# ===== Read raterA_summary.csv and produce paper-ready table + charts =====
# Inputs (one or both):
#   - outputs/scoring_logs/raterA_summary.csv     (per-model means)
#   - outputs/scoring_logs/raterA_scores.csv      (optional: raw rows -> SEMs)
#
# Outputs:
#   - outputs/tables/raterA_summary_percent.tex   (LaTeX table, 0–100%)
#   - outputs/tables/raterA_summary_percent.csv   (CSV table, 0–100%)
#   - outputs/plots_paper/*.png                   (bar charts, 0–100%)

import os, math
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

LOG_DIR   = "outputs/scoring_logs"
TABLE_DIR = "outputs/tables"
PLOT_DIR  = "outputs/plots_paper"
os.makedirs(TABLE_DIR, exist_ok=True)
os.makedirs(PLOT_DIR, exist_ok=True)

summary_path = f"{LOG_DIR}/raterA_summary.csv"
scores_path  = f"{LOG_DIR}/raterA_scores.csv"  # optional
metrics = ["accuracy","cultural_relevance","language_accessibility","bias_avoidance"]

# ---- 1) Load per-model means
if not os.path.exists(summary_path):
    raise FileNotFoundError(f"Not found: {summary_path}. Create it first (your previous step).")
means = pd.read_csv(summary_path)

# Tidy: expect either 'model' as index or column
if "model" not in means.columns:
    means = means.rename_axis("model").reset_index()
means = means.set_index("model")[metrics].astype(float)

# Compute total if missing
if "total_score" not in means.columns:
    means["total_score"] = means[metrics].sum(axis=1)

# ---- 2) Optional: compute SEMs from raw log if available
def sem(x):
    x = pd.to_numeric(x, errors="coerce").dropna()
    return np.nan if len(x) < 2 else x.std(ddof=1) / (len(x) ** 0.5)

sems = None
if os.path.exists(scores_path):
    raw = pd.read_csv(scores_path)
    if "model" in raw.columns and all(m in raw.columns for m in metrics):
        # per-model SEMs
        sems = (raw
                .groupby("model")[metrics]
                .agg(sem)
                .astype(float))
        sems["total_score"] = raw[metrics].sum(axis=1).groupby(raw["model"]).agg(sem).astype(float)

# ---- 3) Convert means/SEMs to percent (0–100)
max_map = {"accuracy":3, "cultural_relevance":3, "language_accessibility":3, "bias_avoidance":3, "total_score":12}
means_pct = means.copy()
for c in list(max_map.keys()):
    if c in means_pct.columns:
        means_pct[c] = means_pct[c] / max_map[c] * 100.0

sems_pct = None
if sems is not None:
    sems_pct = sems.copy()
    for c in list(max_map.keys()):
        if c in sems_pct.columns:
            sems_pct[c] = sems_pct[c] / max_map[c] * 100.0

# ---- 4) Export LaTeX + CSV tables (rounded)
table = means_pct.round(1)[["accuracy","cultural_relevance","language_accessibility","bias_avoidance","total_score"]]
table = table.rename(columns={
    "accuracy":"Accuracy (%)",
    "cultural_relevance":"Cultural rel. (%)",
    "language_accessibility":"Language access. (%)",
    "bias_avoidance":"Bias avoidance (%)",
    "total_score":"Total (%)"
})
table.to_csv(f"{TABLE_DIR}/raterA_summary_percent.csv")

latex = table.to_latex(escape=True, bold_rows=False, column_format="lccccc", caption=(
    "Per-model human ratings from rater A, expressed as percentages (0--100). "
    "Criteria are scored on a 0--3 rubric; Total is the sum (0--12) mapped to percent."
), label="tab:raterA_summary", index=True)
with open(f"{TABLE_DIR}/raterA_summary_percent.tex","w", encoding="utf-8") as f:
    f.write(latex)

print("Saved table:")
print(f"  {TABLE_DIR}/raterA_summary_percent.csv")
print(f"  {TABLE_DIR}/raterA_summary_percent.tex")

# ---- 5) Paper-ready plots (pure matplotlib, one chart per figure, no explicit colors)
def bar_with_err(series, yerr=None, ylabel="", title="", fname="plot.png", ylim=(0,100)):
    fig = plt.figure(figsize=(6,4.5))
    x = np.arange(len(series.index))
    plt.bar(x, series.values, yerr=(None if yerr is None else yerr.values), capsize=4)
    plt.xticks(x, series.index, rotation=15)
    if ylim: plt.ylim(*ylim)
    plt.ylabel(ylabel)
    plt.title(title)
    plt.grid(axis="y", linestyle="--", alpha=0.4)
    plt.tight_layout()
    out = os.path.join(PLOT_DIR, fname)
    plt.savefig(out, dpi=200)
    plt.show()
    plt.close(fig)
    return out

# One figure per metric (means ± SEM if available)
for m in ["accuracy","cultural_relevance","language_accessibility","bias_avoidance","total_score"]:
    if m in means_pct.columns:
        bar_with_err(
            means_pct[m],
            (None if sems_pct is None or m not in sems_pct.columns else sems_pct[m]),
            ylabel=f"{m.replace('_',' ').title()} (%)",
            title=f"{m.replace('_',' ').title()} — mean" + (" ± SEM" if sems_pct is not None else ""),
            fname=f"bar_{m}_percent.png"
        )

# ---- 6) Optional compact table printout for notebook preview
print("\nPer-model means (%, rounded):")
display(table)
if sems_pct is not None:
    print("\nPer-model SEMs (%, rounded):")
    display(sems_pct.round(1))

# === Pilot scoring demo: make a paper-ready table + figure, with clear notes ===
# What this cell does:
# 1) Reads per-model means from raterA_summary.csv (required).
# 2) Reads raw rows from raterA_scores.csv (optional) to compute SEMs.
# 3) Converts everything to percent (0–100).
# 4) Detects uniform pilot scoring -> prints an explicit note for your paper.
# 5) Exports a LaTeX/CSV table + a simple bar chart for "Total (%)".
# 6) Writes a short Markdown snippet you can paste into Methods/Results.

import os, math, json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

LOG_DIR   = "outputs/scoring_logs"
TABLE_DIR = "outputs/tables"
PLOT_DIR  = "outputs/plots_paper"
NOTE_DIR  = "outputs/notes"
os.makedirs(TABLE_DIR, exist_ok=True)
os.makedirs(PLOT_DIR, exist_ok=True)
os.makedirs(NOTE_DIR, exist_ok=True)

summary_path = f"{LOG_DIR}/raterA_summary.csv"   # REQUIRED (means by model)
scores_path  = f"{LOG_DIR}/raterA_scores.csv"     # OPTIONAL (raw rows -> SEMs)

metrics = ["accuracy","cultural_relevance","language_accessibility","bias_avoidance"]
max_map = {"accuracy":3, "cultural_relevance":3, "language_accessibility":3, "bias_avoidance":3, "total_score":12}

# ----- STEP 1: load per-model means (required) -----
if not os.path.exists(summary_path):
    raise FileNotFoundError(f"Missing {summary_path}. Create it first from your rater log.")

means = pd.read_csv(summary_path)
if "model" not in means.columns:
    means = means.rename_axis("model").reset_index()
means = means.set_index("model")[metrics].astype(float)
if "total_score" not in means.columns:
    means["total_score"] = means[metrics].sum(axis=1)

# ----- STEP 2: load raw rows (optional) and compute SEMs -----
def sem(x):
    x = pd.to_numeric(x, errors="coerce").dropna()
    return np.nan if len(x) < 2 else x.std(ddof=1) / (len(x) ** 0.5)

sems = None
n_per_model = None
uniform_within_model = {}
if os.path.exists(scores_path):
    raw = pd.read_csv(scores_path)
    if "model" in raw.columns and all(m in raw.columns for m in metrics):
        # per-model SEMs for each metric and for total
        sems = raw.groupby("model")[metrics].agg(sem).astype(float)
        sems["total_score"] = raw[metrics].sum(axis=1).groupby(raw["model"]).agg(sem).astype(float)
        # counts and uniformity check
        n_per_model = raw.groupby("model").size().to_dict()
        for mdl, g in raw.groupby("model"):
            # if variance is zero across all four metrics for that model, mark uniform
            v = g[metrics].var(numeric_only=True, ddof=1)
            uniform_within_model[mdl] = bool((v.fillna(0) == 0).all())

# ----- STEP 3: convert means/SEMs to percent (0–100) -----
means_pct = means.copy()
for c in list(max_map.keys()):
    if c in means_pct.columns:
        means_pct[c] = means_pct[c] / max_map[c] * 100.0

sems_pct = None
if sems is not None:
    sems_pct = sems.copy()
    for c in list(max_map.keys()):
        if c in sems_pct.columns:
            sems_pct[c] = sems_pct[c] / max_map[c] * 100.0

# ----- STEP 4: detect identical averages across models (pilot uniform scoring) -----
identical_across_models = {}
for col in ["accuracy","cultural_relevance","language_accessibility","bias_avoidance","total_score"]:
    if col in means_pct.columns:
        vals = means_pct[col].dropna().round(6).values
        identical_across_models[col] = (len(vals) > 0 and (np.max(vals) - np.min(vals) == 0.0))

# Build a human-readable note
note_lines = []
note_lines.append("**Pilot scoring demonstration (not final results).**")
if all(identical_across_models.values()):
    note_lines.append("- All three models show identical per-metric averages in this pilot, because the pilot items were scored uniformly.")
else:
    diffs = ", ".join([k for k, same in identical_across_models.items() if not same])
    note_lines.append(f"- Some metrics already differ across models in the pilot: {diffs}.")

if sems_pct is None or float(np.nanmax(sems_pct.values)) == 0.0:
    note_lines.append("- SEMs are 0.0 because there is no within-model variability yet (very small n and/or identical scores).")
else:
    note_lines.append("- SEMs are >0 for some metrics, indicating within-model variability.")

if n_per_model:
    note_lines.append("- Pilot n per model: " + ", ".join([f"{k}: {v}" for k,v in n_per_model.items()]))

pilot_note = "\n".join(note_lines)
print(pilot_note)

# Save the note for reuse in your paper
with open(f"{NOTE_DIR}/pilot_scoring_note.md","w",encoding="utf-8") as f:
    f.write(pilot_note)

# ----- STEP 5: export LaTeX/CSV table and make one simple figure -----
# Table (rounded, percent)
table = means_pct.round(1)[["accuracy","cultural_relevance","language_accessibility","bias_avoidance","total_score"]]
table = table.rename(columns={
    "accuracy":"Accuracy (%)",
    "cultural_relevance":"Cultural rel. (%)",
    "language_accessibility":"Language access. (%)",
    "bias_avoidance":"Bias avoidance (%)",
    "total_score":"Total (%)"
})
table_csv = f"{TABLE_DIR}/pilot_demo_summary_percent.csv"
table_tex = f"{TABLE_DIR}/pilot_demo_summary_percent.tex"
table.to_csv(table_csv)

latex = table.to_latex(escape=True, bold_rows=False, column_format="lccccc", caption=(
    "Pilot scoring demonstration (not final results). Scores are means per model, expressed as percentages (0--100). "
    "Criteria were rated on a 0--3 rubric; Total is the sum (0--12) mapped to percent. "
    "Identical averages across models reflect uniform pilot scoring; SEMs are zero with small $n$."
), label="tab:pilot_demo_summary", index=True)
with open(table_tex,"w",encoding="utf-8") as f:
    f.write(latex)

print("Saved table:")
print(" ", table_csv)
print(" ", table_tex)

# Figure: bar chart for Total (%) with optional SEM
yerr = None
if sems_pct is not None and "total_score" in sems_pct.columns:
    yerr = sems_pct["total_score"]

fig = plt.figure(figsize=(6,4.5))
x = np.arange(len(means_pct.index))
vals = means_pct["total_score"].values
plt.bar(x, vals, yerr=(None if yerr is None else yerr.values), capsize=4)
plt.xticks(x, means_pct.index, rotation=15)
plt.ylim(0, 100)
plt.ylabel("Total (%)")
plt.title("Pilot scoring demonstration — Total (mean" + (" ± SEM" if yerr is not None else "") + ")\n"
          "Identical bars reflect uniform pilot scoring; not final results.")
plt.grid(axis="y", linestyle="--", alpha=0.4)
plt.tight_layout()
fig_path = f"{PLOT_DIR}/pilot_demo_total_percent.png"
plt.savefig(fig_path, dpi=200)
plt.show()
plt.close(fig)

print("Saved figure:")
print(" ", fig_path)

# ----- STEP 6: quick notebook preview -----
print("\nPer-model means (%, rounded):")
display(table)
if sems_pct is not None:
    print("\nPer-model SEMs (%, rounded):")
    display(sems_pct.round(1))

# === Equity add-ons: demographic slices, cultural/safety rates, readability, top keywords ===
import os, re, math
import numpy as np
import pandas as pd
from collections import Counter

# ---------- Helpers
def load_scoring():
    for p in ["./outputs/scoring_template_all_models_DEV.csv",
              "./outputs/scoring_template_all_models.csv"]:
        if os.path.exists(p):
            df = pd.read_csv(p)
            df["source_file"] = os.path.basename(p)
            return df
    raise FileNotFoundError("No scoring file found in ./outputs/.")

def parse_context_fields(s):
    out = {"province": None, "sex": None, "age_group": None, "depression_severity": None}
    if isinstance(s, str):
        for part in s.split(";"):
            part = part.strip()
            if "=" in part:
                k,v = part.split("=",1)
                out[k.strip()] = v.strip()
    return out

def total_score_row(row):
    cols = ["accuracy","cultural_relevance","language_accessibility","bias_avoidance"]
    vals = []
    for c in cols:
        try:
            vals.append(float(row.get(c, np.nan)))
        except Exception:
            vals.append(np.nan)
    if any(pd.isna(vals)):
        return np.nan
    return float(np.nansum(vals))

def sem(x):
    x = pd.Series(x).dropna().astype(float)
    if len(x) < 2:
        return np.nan
    return x.std(ddof=1) / np.sqrt(len(x))

def contains_any(text, patterns):
    if not isinstance(text, str):
        return False
    t = text.lower()
    return any(p in t for p in patterns)

def safe_rate(series_bool):
    s = pd.Series(series_bool)
    denom = (s.notna()).sum()
    return float(s.sum())/denom if denom>0 else np.nan

def avg_sentence_len(text):
    if not isinstance(text, str) or not text.strip():
        return np.nan
    sents = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]
    if not sents:
        return np.nan
    words = sum(len(sent.split()) for sent in sents)
    return words / len(sents)

def avg_chars_per_word(text):
    if not isinstance(text, str) or not text.strip():
        return np.nan
    words = re.findall(r"[A-Za-zÀ-ÖØ-öø-ÿ']+", text)
    if not words:
        return np.nan
    return sum(len(w) for w in words) / len(words)

def top_keywords(texts, topn=12):
    counter = Counter()
    for t in texts:
        if not isinstance(t, str):
            continue
        # light stopword filter by length; keeps cultural terms
        tokens = [w for w in re.findall(r"[A-Za-zÀ-ÖØ-öø-ÿ']+", t.lower()) if len(w) > 3]
        counter.update(tokens)
    return counter.most_common(topn)

# ---------- Load + prep
df = load_scoring()
df["text"] = df.get("clean_response", df.get("response",""))
if "total_score" not in df.columns:
    df["total_score"] = df.apply(total_score_row, axis=1)

parsed = df["context"].apply(parse_context_fields).apply(pd.Series)
for c in ["province","sex","age_group","depression_severity"]:
    if c not in parsed.columns:
        parsed[c] = None
df = pd.concat([df, parsed], axis=1)

metrics = ["accuracy","cultural_relevance","language_accessibility","bias_avoidance"]
scored = df.dropna(subset=metrics, how="any").copy()
for m in metrics:
    scored[m] = pd.to_numeric(scored[m], errors="coerce")

models = sorted(df["model"].dropna().unique().tolist())
os.makedirs("./outputs/tables", exist_ok=True)

print("Using file:", df["source_file"].iloc[0] if "source_file" in df.columns else "N/A")
print(f"N={len(df)} responses across {len(models)} models; scored subset n={len(scored)}")

# ---------- Per-model means & SEMs (scored subset)
if len(scored):
    agg_mean = scored.groupby("model")[metrics+["total_score"]].mean().round(3)
    agg_sem  = scored.groupby("model")[metrics+["total_score"]].apply(lambda g: g.apply(sem)).round(3)
    agg_mean.to_csv("./outputs/tables/per_model_means.csv")
    agg_sem.to_csv("./outputs/tables/per_model_sems.csv")
    print("\n=== Per-model means ===")
    print(agg_mean)
    print("\n=== Per-model SEMs ===")
    print(agg_sem)
else:
    print("\n(No scored rows yet — add more manual ratings to unlock model comparisons.)")

# ---------- Demographic slices (province / sex / age_group)
def slice_scores(group_cols):
    if scored.empty:
        return pd.DataFrame()
    g = scored.groupby(["model"] + group_cols)[metrics+["total_score"]].mean().round(3)
    return g.reset_index()

slice_df = pd.concat([
    slice_scores(["province"]),
    slice_scores(["sex"]),
    slice_scores(["age_group"])
], axis=0, ignore_index=True)

if not slice_df.empty:
    slice_df.to_csv("./outputs/tables/demographic_slice_scores.csv", index=False)
    print("\nSaved demographic slice scores → ./outputs/tables/demographic_slice_scores.csv")
    print(slice_df.head(10))
else:
    print("\n(No demographic slice table because there are no scored rows.)")

# ---------- Cultural markers, crisis guidance, disclaimers
CULTURAL_MARKERS = [
    "elder","elders","two-spirit","two spirit","first nations","indigenous","inuit","métis","metis",
    "community","ceremony","ceremonies","traditional","culture","language","land","healing circle",
    "smudging","sweat lodge","cultural safety","trauma-informed","colonization","residential school",
    "lgbtq","2slgbtq","two-spirit"
]
CRISIS_MARKERS = [
    "emergency","immediate danger","call your local emergency number","crisis line",
    "seek help immediately","nearest emergency","call a crisis","if you feel unsafe"
]
DISCLAIMER_MARKERS = [
    "i can't provide a diagnosis","i cannot provide a diagnosis","i’m not a doctor","i am not a doctor",
    "this is not medical advice","cannot provide medical advice","i can't give medical advice",
    "consult a healthcare professional","talk to your doctor","speak with your doctor"
]

def marker_rate(df_in, markers):
    return df_in.groupby("model")["text"].apply(lambda s: safe_rate(s.apply(lambda t: contains_any(t, markers)))).round(3)

rates_df = pd.concat([
    marker_rate(df, CULTURAL_MARKERS).rename("cultural_marker_rate"),
    marker_rate(df, CRISIS_MARKERS).rename("crisis_guidance_rate"),
    marker_rate(df, DISCLAIMER_MARKERS).rename("disclaimer_or_referral_rate")
], axis=1).sort_index()

rates_df.to_csv("./outputs/tables/cultural_safety_disclaimer_rates.csv")
print("\n=== Cultural/Safety/Disclaimer rates (any occurrence) ===")
print(rates_df)

# ---------- Readability proxies
rows = []
for m, texts in df.groupby("model")["text"]:
    asl = np.nanmean([avg_sentence_len(t) for t in texts])
    cpw = np.nanmean([avg_chars_per_word(t) for t in texts])
    rows.append({"model": m,
                 "avg_sentence_len_words": round(asl,3) if isinstance(asl,(int,float)) else np.nan,
                 "avg_chars_per_word": round(cpw,3) if isinstance(cpw,(int,float)) else np.nan})
read_df = pd.DataFrame(rows).set_index("model")
read_df.to_csv("./outputs/tables/readability_by_model.csv")
print("\n=== Readability proxies ===")
print(read_df)

# ---------- Top keywords (simple, top 12 per model)
kw_rows = []
for m in models:
    m_texts = df.loc[df["model"]==m, "text"]
    for w, c in top_keywords(m_texts, topn=12):
        kw_rows.append({"model": m, "token": w, "count": c})
kw_df = pd.DataFrame(kw_rows)
kw_df.to_csv("./outputs/tables/top_keywords_by_model.csv", index=False)
print("\nTop keywords (top 6 shown for each):")
for m in models:
    print(m, "→", kw_df[kw_df.model==m].head(6)[["token","count"]].to_dict("records"))

# ---------- Paper-ready highlights (plain text)
bullets = []
bullets.append(f"N={len(df)} responses across {len(models)} models; scored subset n={len(scored)}.")
if len(scored):
    best_model = scored.groupby("model")["total_score"].mean().idxmax()
    best_mean  = scored.groupby("model")["total_score"].mean().max()
    bullets.append(f"Highest mean total_score (prelim): {best_model} = {best_mean:.2f}.")
for m in rates_df.index:
    bullets.append(f"{m}: cultural {rates_df.loc[m,'cultural_marker_rate']:.0%}, "
                   f"crisis {rates_df.loc[m,'crisis_guidance_rate']:.0%}, "
                   f"disclaimer {rates_df.loc[m,'disclaimer_or_referral_rate']:.0%}.")
for m in read_df.index:
    bullets.append(f"{m}: sentence len≈{read_df.loc[m,'avg_sentence_len_words']}, "
                   f"chars/word≈{read_df.loc[m,'avg_chars_per_word']}.")

summary_path = "./outputs/tables/paper_summary_points.txt"
with open(summary_path, "w", encoding="utf-8") as f:
    f.write("Paper-ready highlights:\n- " + "\n- ".join(bullets) + "\n")

print("\nSaved highlights →", summary_path)
print("\n=== Highlights ===")
for b in bullets:
    print("-", b)

# ===================== Paper-ready equity diagnostics =====================
# Outputs:
#  - tables/*.csv and *.tex
#  - plots_paper_equity/*.png
# Notes:
#  - Treat as preliminary: n is small. Use later with full scored set.

import os, re, math
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# -------------------- CONFIG --------------------
IN_DEV  = "./outputs/scoring_template_all_models_DEV.csv"
IN_MAIN = "./outputs/scoring_template_all_models.csv"

TABLE_DIR = "./outputs/tables"
PLOT_DIR  = "./outputs/plots_paper_equity"
os.makedirs(TABLE_DIR, exist_ok=True)
os.makedirs(PLOT_DIR, exist_ok=True)

metrics = ["accuracy","cultural_relevance","language_accessibility","bias_avoidance"]
max_map = {"accuracy":3, "cultural_relevance":3, "language_accessibility":3, "bias_avoidance":3, "total_score":12}

# -------------------- LOAD --------------------
path = IN_DEV if os.path.exists(IN_DEV) else IN_MAIN
df = pd.read_csv(path)
resp_col = "clean_response" if "clean_response" in df.columns else "response"
df["text"] = df[resp_col].astype(str)

# total_score (if needed)
if "total_score" not in df.columns:
    df["total_score"] = df[metrics].apply(pd.to_numeric, errors="coerce").sum(axis=1, min_count=1)

# parse demographics
def parse_ctx(s):
    d={}
    if isinstance(s,str):
        for part in s.split(";"):
            if "=" in part:
                k,v = [t.strip() for t in part.split("=",1)]
                d[k]=v
    return pd.Series({
        "province": d.get("province","UNK"),
        "sex": d.get("sex","UNK"),
        "age_group": d.get("age_group","UNK")
    })

ctx = df["context"].apply(parse_ctx)
df = pd.concat([df, ctx], axis=1)

# keep scored rows for metrics summaries
scored = df.dropna(subset=metrics, how="any").copy()
for m in metrics:
    scored[m] = pd.to_numeric(scored[m], errors="coerce")

print(f"Using: {path} | N={len(df)} | Scored n={len(scored)}")

# -------------------- HELPERS --------------------
def sem(series):
    x = pd.to_numeric(series, errors="coerce").dropna()
    return np.nan if len(x)<2 else x.std(ddof=1)/np.sqrt(len(x))

def to_percent_table(means_df):
    out = means_df.copy()
    for c in ["accuracy","cultural_relevance","language_accessibility","bias_avoidance","total_score"]:
        if c in out.columns:
            out[c] = out[c] / max_map[c] * 100.0
    return out

def bar(series, ylabel, title, fname, ylim=None, yerr=None):
    fig = plt.figure(figsize=(6,4.5))
    x = np.arange(len(series.index))
    plt.bar(x, series.values, yerr=(None if yerr is None else yerr.values), capsize=4)
    plt.xticks(x, series.index, rotation=15)
    if ylim: plt.ylim(*ylim)
    plt.ylabel(ylabel)
    plt.title(title)
    plt.grid(axis="y", linestyle="--", alpha=0.4)
    plt.tight_layout()
    out = os.path.join(PLOT_DIR, fname)
    plt.savefig(out, dpi=200)
    plt.close(fig)
    print("Saved:", out)

def contains_any(text, patterns):
    t = text.lower()
    return any(p in t for p in patterns)

# -------------------- (1) PER-MODEL MEANS ± SEM (0–100%) --------------------
if not scored.empty:
    means = scored.groupby("model")[metrics+["total_score"]].mean()
    sems  = scored.groupby("model")[metrics+["total_score"]].agg(sem)
    means_pct = to_percent_table(means).round(1)
    sems_pct  = to_percent_table(sems).round(1)

    means_pct.to_csv(f"{TABLE_DIR}/equity_per_model_means_percent.csv")
    sems_pct.to_csv(f"{TABLE_DIR}/equity_per_model_sems_percent.csv")

    # LaTeX table (means only)
    tex = (means_pct.rename(columns={
            "accuracy":"Accuracy (%)",
            "cultural_relevance":"Cultural rel. (%)",
            "language_accessibility":"Language access. (%)",
            "bias_avoidance":"Bias avoidance (%)",
            "total_score":"Total (%)"
          })
          .to_latex(index=True, escape=True, column_format="lccccc",
                    caption=("Per-model human ratings (preliminary; 0–100%). "
                             "Small scored subset; treat as descriptive."),
                    label="tab:equity_means"))
    with open(f"{TABLE_DIR}/equity_per_model_means_percent.tex","w",encoding="utf-8") as f:
        f.write(tex)

    # plot Total (%) bar with SEM
    bar(means_pct["total_score"], "Total (%)",
        "Per-model total score (mean ± SEM, preliminary)",
        "equity_total_percent.png", ylim=(0,100), yerr=sems_pct["total_score"])
else:
    print("No scored rows yet: skipping per-model means/SEMs.")

# -------------------- (2) CULTURAL / CRISIS / DISCLAIMER RATES -------------
CULTURAL_MARKERS = [
    "elder","elders","two-spirit","two spirit","first nations","indigenous","inuit","métis","metis",
    "community","ceremony","ceremonies","traditional","culture","language","land","healing circle",
    "smudging","sweat lodge","cultural safety","trauma-informed","colonization","residential school"
]
CRISIS_MARKERS = [
    "emergency","immediate danger","call your local emergency number","crisis line",
    "seek help immediately","nearest emergency","call a crisis","if you feel unsafe"
]
DISCLAIMER_MARKERS = [
    "i can't provide a diagnosis","i cannot provide a diagnosis","i’m not a doctor","i am not a doctor",
    "this is not medical advice","cannot provide medical advice","i can't give medical advice",
    "consult a healthcare professional","talk to your doctor","speak with your doctor"
]

def rate(df_in, markers):
    g = df_in.groupby("model")["text"].apply(lambda s: (s.apply(lambda t: contains_any(t, markers)).sum())/len(s))
    return (g*100.0).round(1)

rates = pd.DataFrame({
    "Cultural marker (%)": rate(df, CULTURAL_MARKERS),
    "Crisis guidance (%)": rate(df, CRISIS_MARKERS),
    "Disclaimer/referral (%)": rate(df, DISCLAIMER_MARKERS),
})
rates.to_csv(f"{TABLE_DIR}/equity_rates_percent.csv")

# plots (one per metric)
for col, fname, ttl in [
    ("Cultural marker (%)",   "equity_cultural_rate.png",   "Cultural marker rate (any occurrence)"),
    ("Crisis guidance (%)",   "equity_crisis_rate.png",     "Crisis guidance rate (any occurrence)"),
    ("Disclaimer/referral (%)","equity_disclaimer_rate.png","Disclaimer/referral rate (any occurrence)"),
]:
    bar(rates[col], col, ttl + " — preliminary", fname, ylim=(0,100))

# -------------------- (3) READABILITY (avg sentence len; chars/word) -------
def avg_sentence_len(text):
    sents = [s.strip() for s in re.split(r"[.!?]+", text) if s.strip()]
    if not sents: return np.nan
    return np.mean([len(s.split()) for s in sents])

def avg_chars_per_word(text):
    words = re.findall(r"[A-Za-zÀ-ÖØ-öø-ÿ']+", text)
    if not words: return np.nan
    return np.mean([len(w) for w in words])

read_rows = []
for mdl, texts in df.groupby("model")["text"]:
    read_rows.append({
        "model": mdl,
        "Avg sentence len (words)": np.nanmean([avg_sentence_len(t) for t in texts]),
        "Avg chars per word": np.nanmean([avg_chars_per_word(t) for t in texts]),
    })
read_df = pd.DataFrame(read_rows).set_index("model").round(2)
read_df.to_csv(f"{TABLE_DIR}/equity_readability.csv")

bar(read_df["Avg sentence len (words)"], "Words", "Average sentence length (preliminary)", "equity_read_sentlen.png")
bar(read_df["Avg chars per word"], "Chars", "Average characters per word (preliminary)", "equity_read_cpw.png")

# -------------------- (4) DEMOGRAPHIC SLICES (tables + optional bars) ------
def slice_scores(group_col):
    if scored.empty: return None
    g = (scored.groupby(["model", group_col])[metrics+["total_score"]]
         .mean().round(2).reset_index())
    csv_path = f"{TABLE_DIR}/equity_slice_{group_col}.csv"
    g.to_csv(csv_path, index=False)
    print("Saved:", csv_path)
    return g

slice_prov = slice_scores("province")
slice_sex  = slice_scores("sex")

# Optional: grouped bars for province totals (Total score, %)
if slice_prov is not None and not slice_prov.empty:
    tmp = slice_prov.pivot(index="province", columns="model", values="total_score")
    tmp_pct = (tmp / max_map["total_score"] * 100.0).round(1)
    for prov in tmp_pct.index:
        # one small bar per province across models
        bar(tmp_pct.loc[prov], "Total (%)", f"Total (%) by model — {prov} (preliminary)",
            f"equity_slice_total_{prov}.png", ylim=(0,100))

# -------------------- (5) TOP KEYWORDS PER MODEL (compact table) ----------
from collections import Counter
def top_keywords(texts, topn=12):
    c = Counter()
    for t in texts:
        toks = [w for w in re.findall(r"[A-Za-zÀ-ÖØ-öø-ÿ']+", str(t).lower()) if len(w)>3]
        c.update(toks)
    return c.most_common(topn)

kw_rows = []
for mdl in sorted(df["model"].unique()):
    for w,cnt in top_keywords(df.loc[df["model"]==mdl, "text"], topn=12):
        kw_rows.append({"model": mdl, "token": w, "count": int(cnt)})
kw_df = pd.DataFrame(kw_rows)
kw_df.to_csv(f"{TABLE_DIR}/equity_top_keywords.csv", index=False)

# Make a small LaTeX for top-8 terms per model (appendix)
latex_blocks = []
for mdl in sorted(df["model"].unique()):
    sub = kw_df[kw_df.model==mdl].head(8)
    t = sub[["token","count"]].to_latex(index=False, escape=True,
                                        caption=f"Top keywords — {mdl} (preliminary)", label=None)
    latex_blocks.append(t)
with open(f"{TABLE_DIR}/equity_top_keywords_tables.tex","w",encoding="utf-8") as f:
    f.write("\n\n".join(latex_blocks))

print("\nDone. Key files saved to:")
print("  Tables:", TABLE_DIR)
print("  Figures:", PLOT_DIR)

# ===================== Paper-ready equity diagnostics (SHOW FIRST, then save) =====================
import os, re, math
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# pretty display if in Jupyter
try:
    from IPython.display import display
except Exception:
    def display(x): print(x)

# -------------------- CONFIG --------------------
IN_DEV  = "./outputs/scoring_template_all_models_DEV.csv"
IN_MAIN = "./outputs/scoring_template_all_models.csv"

TABLE_DIR = "./outputs/tables"
PLOT_DIR  = "./outputs/plots_paper_equity"
os.makedirs(TABLE_DIR, exist_ok=True)
os.makedirs(PLOT_DIR, exist_ok=True)

metrics = ["accuracy","cultural_relevance","language_accessibility","bias_avoidance"]
max_map = {"accuracy":3, "cultural_relevance":3, "language_accessibility":3, "bias_avoidance":3, "total_score":12}

# -------------------- LOAD --------------------
path = IN_DEV if os.path.exists(IN_DEV) else IN_MAIN
df = pd.read_csv(path)
resp_col = "clean_response" if "clean_response" in df.columns else "response"
df["text"] = df[resp_col].astype(str)

if "total_score" not in df.columns:
    df["total_score"] = df[metrics].apply(pd.to_numeric, errors="coerce").sum(axis=1, min_count=1)

def parse_ctx(s):
    d={}
    if isinstance(s,str):
        for part in s.split(";"):
            if "=" in part:
                k,v = [t.strip() for t in part.split("=",1)]
                d[k]=v
    return pd.Series({
        "province": d.get("province","UNK"),
        "sex": d.get("sex","UNK"),
        "age_group": d.get("age_group","UNK")
    })

ctx = df["context"].apply(parse_ctx)
df = pd.concat([df, ctx], axis=1)

scored = df.dropna(subset=metrics, how="any").copy()
for m in metrics:
    scored[m] = pd.to_numeric(scored[m], errors="coerce")

print(f"Using: {path} | N={len(df)} | Scored n={len(scored)}")

# -------------------- HELPERS --------------------
def sem(series):
    x = pd.to_numeric(series, errors="coerce").dropna()
    return np.nan if len(x)<2 else x.std(ddof=1)/np.sqrt(len(x))

def to_percent_table(means_df):
    out = means_df.copy()
    for c in ["accuracy","cultural_relevance","language_accessibility","bias_avoidance","total_score"]:
        if c in out.columns:
            out[c] = out[c] / max_map[c] * 100.0
    return out

def bar(series, ylabel, title, fname, ylim=None, yerr=None):
    fig = plt.figure(figsize=(6,4.5))
    x = np.arange(len(series.index))
    plt.bar(x, series.values, yerr=(None if yerr is None else yerr.values), capsize=4)
    plt.xticks(x, series.index, rotation=15)
    if ylim: plt.ylim(*ylim)
    plt.ylabel(ylabel)
    plt.title(title)
    plt.grid(axis="y", linestyle="--", alpha=0.4)
    plt.tight_layout()
    # SHOW first
    plt.show()
    # then SAVE
    out = os.path.join(PLOT_DIR, fname)
    fig.savefig(out, dpi=200)
    plt.close(fig)
    print("Saved figure →", out)

def contains_any(text, patterns):
    t = text.lower()
    return any(p in t for p in patterns)

# -------------------- (1) PER-MODEL MEANS ± SEM (0–100%) --------------------
if not scored.empty:
    means = scored.groupby("model")[metrics+["total_score"]].mean()
    sems  = scored.groupby("model")[metrics+["total_score"]].agg(sem)
    means_pct = to_percent_table(means).round(1)
    sems_pct  = to_percent_table(sems).round(1)

    print("\n=== Per-model means (%, rounded) ===")
    display(means_pct)
    print("\n=== Per-model SEMs (%, rounded) ===")
    display(sems_pct)

    means_pct.to_csv(f"{TABLE_DIR}/equity_per_model_means_percent.csv")
    sems_pct.to_csv(f"{TABLE_DIR}/equity_per_model_sems_percent.csv")
    print("Saved tables →",
          f"{TABLE_DIR}/equity_per_model_means_percent.csv",
          f"{TABLE_DIR}/equity_per_model_sems_percent.csv", sep="\n  ")

    # LaTeX table (means only)
    tex = (means_pct.rename(columns={
            "accuracy":"Accuracy (%)",
            "cultural_relevance":"Cultural rel. (%)",
            "language_accessibility":"Language access. (%)",
            "bias_avoidance":"Bias avoidance (%)",
            "total_score":"Total (%)"
          })
          .to_latex(index=True, escape=True, column_format="lccccc",
                    caption=("Per-model human ratings (preliminary; 0–100%). "
                             "Small scored subset; treat as descriptive."),
                    label="tab:equity_means"))
    with open(f"{TABLE_DIR}/equity_per_model_means_percent.tex","w",encoding="utf-8") as f:
        f.write(tex)
    print("Saved LaTeX →", f"{TABLE_DIR}/equity_per_model_means_percent.tex")

    # plot Total (%) bar with SEM — show then save
    bar(means_pct["total_score"], "Total (%)",
        "Per-model total score (mean ± SEM, preliminary)",
        "equity_total_percent.png", ylim=(0,100), yerr=sems_pct["total_score"])
else:
    print("No scored rows yet: skipping per-model means/SEMs.")

# -------------------- (2) CULTURAL / CRISIS / DISCLAIMER RATES -------------
CULTURAL_MARKERS = [
    "elder","elders","two-spirit","two spirit","first nations","indigenous","inuit","métis","metis",
    "community","ceremony","ceremonies","traditional","culture","language","land","healing circle",
    "smudging","sweat lodge","cultural safety","trauma-informed","colonization","residential school"
]
CRISIS_MARKERS = [
    "emergency","immediate danger","call your local emergency number","crisis line",
    "seek help immediately","nearest emergency","call a crisis","if you feel unsafe"
]
DISCLAIMER_MARKERS = [
    "i can't provide a diagnosis","i cannot provide a diagnosis","i’m not a doctor","i am not a doctor",
    "this is not medical advice","cannot provide medical advice","i can't give medical advice",
    "consult a healthcare professional","talk to your doctor","speak with your doctor"
]

def rate(df_in, markers):
    g = df_in.groupby("model")["text"].apply(lambda s: (s.apply(lambda t: contains_any(t, markers)).sum())/len(s))
    return (g*100.0).round(1)

rates = pd.DataFrame({
    "Cultural marker (%)": rate(df, CULTURAL_MARKERS),
    "Crisis guidance (%)": rate(df, CRISIS_MARKERS),
    "Disclaimer/referral (%)": rate(df, DISCLAIMER_MARKERS),
})

print("\n=== Cultural/Safety/Disclaimer rates (%, any occurrence) ===")
display(rates)

rates.to_csv(f"{TABLE_DIR}/equity_rates_percent.csv")
print("Saved table →", f"{TABLE_DIR}/equity_rates_percent.csv")

# show & save each plot
for col, fname, ttl in [
    ("Cultural marker (%)",   "equity_cultural_rate.png",   "Cultural marker rate (any occurrence)"),
    ("Crisis guidance (%)",   "equity_crisis_rate.png",     "Crisis guidance rate (any occurrence)"),
    ("Disclaimer/referral (%)","equity_disclaimer_rate.png","Disclaimer/referral rate (any occurrence)"),
]:
    bar(rates[col], col, ttl + " — preliminary", fname, ylim=(0,100))

# -------------------- (3) READABILITY (avg sentence len; chars/word) -------
def avg_sentence_len(text):
    sents = [s.strip() for s in re.split(r"[.!?]+", text) if s.strip()]
    if not sents: return np.nan
    return np.mean([len(s.split()) for s in sents])

def avg_chars_per_word(text):
    words = re.findall(r"[A-Za-zÀ-ÖØ-öø-ÿ']+", text)
    if not words: return np.nan
    return np.mean([len(w) for w in words])

read_rows = []
for mdl, texts in df.groupby("model")["text"]:
    read_rows.append({
        "model": mdl,
        "Avg sentence len (words)": np.nanmean([avg_sentence_len(t) for t in texts]),
        "Avg chars per word": np.nanmean([avg_chars_per_word(t) for t in texts]),
    })
read_df = pd.DataFrame(read_rows).set_index("model").round(2)

print("\n=== Readability metrics ===")
display(read_df)

read_df.to_csv(f"{TABLE_DIR}/equity_readability.csv")
print("Saved table →", f"{TABLE_DIR}/equity_readability.csv")

bar(read_df["Avg sentence len (words)"], "Words", "Average sentence length (preliminary)", "equity_read_sentlen.png")
bar(read_df["Avg chars per word"], "Chars", "Average characters per word (preliminary)", "equity_read_cpw.png")

# -------------------- (4) DEMOGRAPHIC SLICES (tables + optional bars) ------
def slice_scores(group_col):
    if scored.empty: return None
    g = (scored.groupby(["model", group_col])[metrics+["total_score"]]
         .mean().round(2).reset_index())
    print(f"\n=== Slice by {group_col} (means) ===")
    display(g)
    csv_path = f"{TABLE_DIR}/equity_slice_{group_col}.csv"
    g.to_csv(csv_path, index=False)
    print("Saved table →", csv_path)
    return g

slice_prov = slice_scores("province")
slice_sex  = slice_scores("sex")

if slice_prov is not None and not slice_prov.empty:
    tmp = slice_prov.pivot(index="province", columns="model", values="total_score")
    tmp_pct = (tmp / max_map["total_score"] * 100.0).round(1)
    print("\n=== Province totals (%, pivot) ===")
    display(tmp_pct)
    for prov in tmp_pct.index:
        bar(tmp_pct.loc[prov], "Total (%)", f"Total (%) by model — {prov} (preliminary)",
            f"equity_slice_total_{prov}.png", ylim=(0,100))

# -------------------- (5) TOP KEYWORDS PER MODEL (compact table) ----------
from collections import Counter
def top_keywords(texts, topn=12):
    c = Counter()
    for t in texts:
        toks = [w for w in re.findall(r"[A-Za-zÀ-ÖØ-öø-ÿ']+", str(t).lower()) if len(w)>3]
        c.update(toks)
    return c.most_common(topn)

kw_rows = []
for mdl in sorted(df["model"].unique()):
    for w,cnt in top_keywords(df.loc[df["model"]==mdl, "text"], topn=12):
        kw_rows.append({"model": mdl, "token": w, "count": int(cnt)})
kw_df = pd.DataFrame(kw_rows)

print("\n=== Top keywords (top 8 per model) ===")
for mdl in sorted(df["model"].unique()):
    sub = kw_df[kw_df.model==mdl].head(8)[["token","count"]]
    print(f"\n{mdl}")
    display(sub)

kw_df.to_csv(f"{TABLE_DIR}/equity_top_keywords.csv", index=False)
print("Saved table →", f"{TABLE_DIR}/equity_top_keywords.csv")

# small LaTeX tables per model (appendix)
latex_blocks = []
for mdl in sorted(df["model"].unique()):
    sub = kw_df[kw_df.model==mdl].head(8)
    t = sub[["token","count"]].to_latex(index=False, escape=True,
                                        caption=f"Top keywords — {mdl} (preliminary)", label=None)
    latex_blocks.append(t)
with open(f"{TABLE_DIR}/equity_top_keywords_tables.tex","w",encoding="utf-8") as f:
    f.write("\n\n".join(latex_blocks))
print("Saved LaTeX →", f"{TABLE_DIR}/equity_top_keywords_tables.tex")

print("\nDone. Key files saved to:")
print("  Tables:", TABLE_DIR)
print("  Figures:", PLOT_DIR)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Data from your results
means = pd.DataFrame({
    'model': ['deepseek-r1:8b', 'llama3.2:latest', 'mistral:7b'],
    'accuracy': [2.308, 2.000, 2.000],
    'cultural_relevance': [1.769, 2.000, 2.000],
    'language_accessibility': [1.538, 3.000, 3.000],
    'bias_avoidance': [1.385, 2.000, 2.000],
    'total_score': [7.0, 9.0, 9.0]
})

safety = pd.DataFrame({
    'model': ['deepseek-r1:8b', 'llama3.2:latest', 'mistral:7b'],
    'cultural_marker_rate': [1.0, 1.0, 1.0],
    'crisis_guidance_rate': [0.309, 0.962, 0.964],
    'disclaimer_rate': [0.002, 0.0, 0.0]
})

readability = pd.DataFrame({
    'model': ['deepseek-r1:8b', 'llama3.2:latest', 'mistral:7b'],
    'avg_sentence_len': [16.281, 14.046, 11.777],
    'avg_chars_per_word': [5.428, 5.287, 5.671]
})

# Set style
sns.set(style="whitegrid")

# --- Plot 1: Mean Scores by Model ---
plt.figure(figsize=(8,6))
means_plot = means.melt(id_vars='model', var_name='metric', value_name='score')
sns.barplot(data=means_plot, x='metric', y='score', hue='model')
plt.title("Mean Equity-Related Scores by Model")
plt.ylabel("Mean Score")
plt.xticks(rotation=45)
plt.legend(title='Model')
plt.tight_layout()
plt.savefig("mean_scores_by_model.png")
plt.show()

# --- Plot 2: Safety/Cultural Marker Rates ---
plt.figure(figsize=(8,6))
safety_plot = safety.melt(id_vars='model', var_name='metric', value_name='rate')
sns.barplot(data=safety_plot, x='metric', y='rate', hue='model')
plt.title("Safety & Cultural Marker Rates by Model")
plt.ylabel("Rate (0–1)")
plt.xticks(rotation=45)
plt.legend(title='Model')
plt.tight_layout()
plt.savefig("safety_rates_by_model.png")
plt.show()

# --- Plot 3: Readability Comparison ---
fig, ax1 = plt.subplots(figsize=(8,6))
sns.barplot(data=readability, x='model', y='avg_sentence_len', color='skyblue', label='Sentence Length (words)', ax=ax1)
ax2 = ax1.twinx()
sns.lineplot(data=readability, x='model', y='avg_chars_per_word', marker='o', color='red', label='Chars/Word', ax=ax2)
ax1.set_ylabel("Sentence Length (words)")
ax2.set_ylabel("Characters per Word")
plt.title("Readability Profiles by Model")
fig.tight_layout()
plt.savefig("readability_profiles_by_model.png")
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Create the data from your provided summary
means_df = pd.DataFrame({
    'model': ['deepseek-r1:8b', 'llama3.2:latest', 'mistral:7b'],
    'accuracy': [2.308, 2.0, 2.0],
    'cultural_relevance': [1.769, 2.0, 2.0],
    'language_accessibility': [1.538, 3.0, 3.0],
    'bias_avoidance': [1.385, 2.0, 2.0],
    'total_score': [7.0, 9.0, 9.0]
})

safety_df = pd.DataFrame({
    'model': ['deepseek-r1:8b', 'llama3.2:latest', 'mistral:7b'],
    'cultural_marker_rate': [1.0, 1.0, 1.0],
    'crisis_guidance_rate': [0.309, 0.962, 0.964],
    'disclaimer_or_referral_rate': [0.002, 0.0, 0.0]
})

readability_df = pd.DataFrame({
    'model': ['deepseek-r1:8b', 'llama3.2:latest', 'mistral:7b'],
    'avg_sentence_len_words': [16.281, 14.046, 11.777],
    'avg_chars_per_word': [5.428, 5.287, 5.671]
})

# 1. Mean scores by model
plt.figure(figsize=(8, 5))
means_df.set_index('model').plot(kind='bar')
plt.title("Mean Scores by Model")
plt.ylabel("Mean Score")
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig("mean_scores_by_model2.png")

plt.show()
plt.close()
# 2. Safety / cultural marker rates
plt.figure(figsize=(8, 5))
safety_df.set_index('model').plot(kind='bar')
plt.title("Safety and Cultural Marker Rates by Model")
plt.ylabel("Rate")
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig("safety_cultural_marker_rates2.png")
plt.close()
plt.show()
# 3. Readability profiles
fig, ax1 = plt.subplots(figsize=(8, 5))
ax2 = ax1.twinx()
width = 0.4

ax1.bar(readability_df['model'], readability_df['avg_sentence_len_words'], width=width, label='Avg Sentence Length (words)', color='skyblue', align='center')
ax2.bar(readability_df['model'], readability_df['avg_chars_per_word'], width=width, label='Avg Chars per Word', color='orange', align='edge')

ax1.set_ylabel("Avg Sentence Length (words)", color='blue')
ax2.set_ylabel("Avg Chars per Word", color='orange')
plt.title("Readability Profiles by Model")
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig("readability_profiles2.png")
plt.show()
plt.close()



#"/mnt/data/mean_scores_by_model.png", "/mnt/data/safety_cultural_marker_rates.png", "/mnt/data/readability_profiles.png"

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import re

# === Load ===
scored_file = "./outputs/scoring_template_all_models_DEV.csv"
df = pd.read_csv(scored_file)
print("Loaded:", scored_file, "rows:", len(df))

# === Ensure numeric metrics + total_score ===
metrics = ["accuracy","cultural_relevance","language_accessibility","bias_avoidance"]
for m in metrics:
    if m not in df.columns:
        raise ValueError(f"Missing metric column: {m}")
    df[m] = pd.to_numeric(df[m], errors="coerce")

if "total_score" not in df.columns:
    df["total_score"] = df[metrics].sum(axis=1, min_count=1)

# === Derive readability if missing ===
def calc_readability(text):
    if not isinstance(text, str) or not text.strip():
        return pd.Series({"avg_sentence_len_words": np.nan, "avg_chars_per_word": np.nan, "response_length": np.nan})
    # sentence split (simple)
    sents = re.split(r"[.!?]+", text)
    words = re.findall(r"\b\w+\b", text)
    avg_sent_len = (sum(len(re.findall(r'\b\w+\b', s)) for s in sents if s.strip()) / max(1, sum(1 for s in sents if s.strip())))
    avg_chars_per_word = (sum(len(w) for w in words) / max(1, len(words)))
    return pd.Series({
        "avg_sentence_len_words": avg_sent_len,
        "avg_chars_per_word": avg_chars_per_word,
        "response_length": len(text)
    })

if "avg_sentence_len_words" not in df.columns or "avg_chars_per_word" not in df.columns or "response_length" not in df.columns:
    base_text_col = "clean_response" if "clean_response" in df.columns else "response"
    print(f"Computing readability from: {base_text_col}")
    df[["avg_sentence_len_words","avg_chars_per_word","response_length"]] = df[base_text_col].apply(calc_readability)

# === Derive safety/cultural markers if missing ===
if "cultural_marker_rate" not in df.columns or "crisis_guidance_rate" not in df.columns or "disclaimer_or_referral_rate" not in df.columns:
    base_text_col = "clean_response" if "clean_response" in df.columns else "response"
    txt = df[base_text_col].fillna("")

    cultural_terms = [
        "first nation", "indigenous", "elder", "ceremon", "language", "land-based",
        "two-spirit", "community", "culture", "traditional"
    ]
    crisis_terms = [
        "seek help", "call", "hotline", "crisis line", "emergency", "911", "988",
        "if you are in immediate danger", "urgent", "lifeline"
    ]
    disclaimer_terms = [
        "not a substitute", "not medical advice", "consult a healthcare professional",
        "talk to your doctor", "seek professional help", "this tool does not"
    ]

    def contains_any(text, terms):
        t = text.lower()
        return any(k in t for k in terms)

    df["cultural_marker_rate"] = txt.apply(lambda t: 1.0 if contains_any(t, cultural_terms) else 0.0)
    df["crisis_guidance_rate"] = txt.apply(lambda t: 1.0 if contains_any(t, crisis_terms) else 0.0)
    df["disclaimer_or_referral_rate"] = txt.apply(lambda t: 1.0 if contains_any(t, disclaimer_terms) else 0.0)

# === Aggregate ===
grp = df.groupby("model")
agg_mean = grp[metrics + ["total_score"]].mean()
agg_sem  = grp[metrics + ["total_score"]].sem()
print("\nPer-model means:\n", agg_mean.round(3))
print("\nPer-model SEMs:\n", agg_sem.round(3))

# === Plot 1: Mean scores by model (± SEM) ===
plt.figure(figsize=(9, 5))
x = np.arange(len(agg_mean.index))
bars = plt.bar(x, agg_mean["total_score"], yerr=agg_sem["total_score"], capsize=4)
plt.xticks(x, agg_mean.index, rotation=15, ha="right")
plt.ylabel("Mean Total Score (0–12)")
plt.title("Mean Total Score by Model (± SEM)")

# value labels
for xi, b in zip(x, bars):
    y = b.get_height()
    plt.text(xi, y + 0.05, f"{y:.2f}", ha="center", va="bottom", fontsize=9)

plt.tight_layout()
plt.show()

# === Plot 2: Safety/Cultural marker rates (stacked bar) ===
rates = grp[["cultural_marker_rate","crisis_guidance_rate","disclaimer_or_referral_rate"]].mean()
models = list(rates.index)
i = np.arange(len(models))

plt.figure(figsize=(9, 5))
bottom = np.zeros(len(models))
for col in ["cultural_marker_rate","crisis_guidance_rate","disclaimer_or_referral_rate"]:
    plt.bar(i, rates[col].values, bottom=bottom, label=col.replace("_"," "))
    bottom += rates[col].values

plt.xticks(i, models, rotation=15, ha="right")
plt.ylabel("Rate")
plt.title("Safety & Cultural Marker Rates by Model")
plt.legend()
plt.tight_layout()
plt.show()

# === Plot 3: Readability profiles (scatter) ===
read_mean = grp[["avg_sentence_len_words","avg_chars_per_word"]].mean().reset_index()
plt.figure(figsize=(8, 5))
for _, row in read_mean.iterrows():
    plt.scatter(row["avg_sentence_len_words"], row["avg_chars_per_word"], s=120, label=row["model"])
plt.xlabel("Avg Sentence Length (words)")
plt.ylabel("Avg Characters per Word")
plt.title("Readability Profiles by Model")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""# 1) Per-metric bars (mean ± SEM) — side-by-side"""

# --- Per-metric mean ± SEM bars ---
import os, numpy as np, pandas as pd, matplotlib.pyplot as plt

PLOT_DIR = "./outputs/plots_paper_equity"
os.makedirs(PLOT_DIR, exist_ok=True)

path = "./outputs/scoring_template_all_models_DEV.csv"
df = pd.read_csv(path)
metrics = ["accuracy","cultural_relevance","language_accessibility","bias_avoidance"]
for m in metrics: df[m] = pd.to_numeric(df[m], errors="coerce")
scored = df.dropna(subset=metrics, how="any")

def sem(s):
    x = pd.to_numeric(s, errors="coerce").dropna()
    return np.nan if len(x)<2 else x.std(ddof=1)/np.sqrt(len(x))

means = scored.groupby("model")[metrics].mean()
sems  = scored.groupby("model")[metrics].agg(sem)

fig, ax = plt.subplots(figsize=(8,5))
x = np.arange(len(metrics))
width = 0.22
models = list(means.index)

for i, mdl in enumerate(models):
    ax.bar(x + i*width, means.loc[mdl].values, width, yerr=sems.loc[mdl].values, capsize=4, label=mdl)

ax.set_xticks(x + width*(len(models)-1)/2)
ax.set_xticklabels(["Accuracy","Cultural rel.","Lang. access.","Bias avoid."], rotation=15)
ax.set_ylabel("Mean score (0–3)")
ax.set_title("Per-metric scores by model (mean ± SEM)")
ax.legend()
ax.grid(axis="y", linestyle="--", alpha=0.35)
plt.tight_layout()
plt.show()
fig.savefig(f"{PLOT_DIR}/per_metric_mean_sem.png", dpi=200)
plt.close(fig)

"""# 2) 100% stacked bars — distribution of 0/1/2/3 ratings"""

# --- 100% stacked distribution of 0/1/2/3 ratings per metric & model ---
import numpy as np, pandas as pd, matplotlib.pyplot as plt

def stacked_dist_plot(metric):
    sub = scored[['model', metric]].dropna().copy()
    sub[metric] = sub[metric].astype(int).clip(0,3)
    counts = sub.groupby(['model', metric]).size().unstack(fill_value=0).reindex(columns=[0,1,2,3])
    props  = counts.div(counts.sum(axis=1), axis=0)

    fig, ax = plt.subplots(figsize=(7,4))
    bottom = np.zeros(len(props))
    idx = np.arange(len(props))
    for val in [0,1,2,3]:
        ax.bar(idx, props[val].values*100, bottom=bottom, label=str(val))
        bottom += props[val].values*100
    ax.set_xticks(idx); ax.set_xticklabels(props.index, rotation=15)
    ax.set_ylabel("% of ratings")
    ax.set_title(f"Rating distribution for '{metric}' (0–3)")
    ax.legend(title="Score", ncol=4, frameon=False)
    ax.set_ylim(0,100); ax.grid(axis="y", linestyle="--", alpha=0.35)
    plt.tight_layout(); plt.show()
    fig.savefig(f"{PLOT_DIR}/dist_scores_{metric}.png", dpi=200); plt.close(fig)

for m in metrics: stacked_dist_plot(m)



"""# 3) Demographic slice heatmaps (province × sex)"""

# --- Heatmaps of Total Score by province × sex (per model) ---
import numpy as np, pandas as pd, matplotlib.pyplot as plt, re

def parse_ctx(s):
    d={}
    if isinstance(s,str):
        for part in s.split(";"):
            if "=" in part:
                k,v = [t.strip() for t in part.split("=",1)]
                d[k]=v
    return pd.Series({"province": d.get("province","UNK"), "sex": d.get("sex","UNK")})

ctx = df["context"].apply(parse_ctx)
dfh = pd.concat([scored, ctx], axis=1)
dfh["total_score"] = dfh[metrics].sum(axis=1)

for mdl, sub in dfh.groupby("model"):
    piv = sub.pivot_table(index="province", columns="sex", values="total_score", aggfunc="mean")
    fig, ax = plt.subplots(figsize=(5.2,4.2))
    im = ax.imshow(piv.values, aspect="auto", cmap="Blues", vmin=0, vmax=12)
    ax.set_xticks(np.arange(len(piv.columns))); ax.set_xticklabels(piv.columns)
    ax.set_yticks(np.arange(len(piv.index)));   ax.set_yticklabels(piv.index)
    for i in range(piv.shape[0]):
        for j in range(piv.shape[1]):
            val = piv.values[i,j]
            if not np.isnan(val):
                ax.text(j, i, f"{val:.1f}", ha="center", va="center", fontsize=9)
    ax.set_title(f"Total score by province × sex — {mdl}")
    fig.colorbar(im, ax=ax, label="Mean total (0–12)")
    plt.tight_layout(); plt.show()
    fig.savefig(f"{PLOT_DIR}/heatmap_total_prov_sex__{mdl.replace(':','_')}.png", dpi=200); plt.close(fig)

"""# 4) Response-length distribution by model"""

# --- Boxplot of response word-count by model ---
resp_col = "clean_response" if "clean_response" in df.columns else "response"
word_counts = df[resp_col].astype(str).str.split().apply(len)
df_len = pd.DataFrame({"model": df["model"], "words": word_counts})

fig, ax = plt.subplots(figsize=(7,4))
data = [df_len.loc[df_len.model==m, "words"].dropna() for m in df_len.model.unique()]
ax.boxplot(data, tick_labels=list(df_len.model.unique()), showfliers=False)
ax.set_ylabel("Words per response"); ax.set_xlabel("Model")
ax.set_title("Response length by model"); ax.grid(axis="y", linestyle="--", alpha=0.35)
plt.tight_layout(); plt.show()
fig.savefig(f"{PLOT_DIR}/box_response_length_by_model.png", dpi=200); plt.close(fig)

"""# 5) Correlations: metrics vs. readability & safety markers"""

# --- Correlation heatmap (metrics vs readability & markers) ---
import numpy as np, pandas as pd, matplotlib.pyplot as plt, re

# compute readability if not present
def sent_len(text):
    sents = [s.strip() for s in re.split(r"[.!?]+", str(text)) if s.strip()]
    return np.mean([len(s.split()) for s in sents]) if sents else np.nan
def chars_per_word(text):
    words = re.findall(r"[A-Za-zÀ-ÖØ-öø-ÿ']+", str(text))
    return np.mean([len(w) for w in words]) if words else np.nan

if "avg_sentence_len_words" not in df.columns:
    df["avg_sentence_len_words"] = df[resp_col].apply(sent_len)
if "avg_chars_per_word" not in df.columns:
    df["avg_chars_per_word"] = df[resp_col].apply(chars_per_word)

corr_cols = metrics + ["total_score","avg_sentence_len_words","avg_chars_per_word",
                       "cultural_marker_rate","crisis_guidance_rate","disclaimer_or_referral_rate"]
avail = [c for c in corr_cols if c in df.columns]
corr = df[avail].apply(pd.to_numeric, errors="coerce").corr()

fig, ax = plt.subplots(figsize=(7.5,6))
im = ax.imshow(corr.values, cmap="coolwarm", vmin=-1, vmax=1)
ax.set_xticks(np.arange(len(corr.columns))); ax.set_xticklabels(corr.columns, rotation=45, ha="right")
ax.set_yticks(np.arange(len(corr.index)));   ax.set_yticklabels(corr.index)
for i in range(corr.shape[0]):
    for j in range(corr.shape[1]):
        ax.text(j, i, f"{corr.values[i,j]:.2f}", ha="center", va="center", fontsize=8, color="black")
ax.set_title("Correlation matrix (metrics, readability, markers)")
fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04, label="Pearson r")
plt.tight_layout(); plt.show()
fig.savefig(f"{PLOT_DIR}/correlation_metrics_readability_markers.png", dpi=200); plt.close(fig)

"""# 6) “Waterfall” composition of total score per model"""

# --- Stacked bars: contribution of criteria to total ---
contrib = scored.groupby("model")[metrics].mean()
fig, ax = plt.subplots(figsize=(7.2,4.8))
bottom = np.zeros(len(contrib))
x = np.arange(len(contrib.index))
for m in metrics:
    ax.bar(x, contrib[m].values, bottom=bottom, label=m.replace("_"," "))
    bottom += contrib[m].values
ax.set_xticks(x); ax.set_xticklabels(contrib.index, rotation=15)
ax.set_ylabel("Mean contribution (0–3 each)")
ax.set_title("Total score composition by model (mean)")
#ax.legend(ncol=2, frameon=False); ax.grid(axis="y", linestyle="--", alpha=0.35)
ax.legend(ncol=2, frameon=False, bbox_to_anchor=(0.5, -0.15), loc="upper center")


plt.tight_layout(); plt.show()
fig.savefig(f"{PLOT_DIR}/stacked_total_composition.png", dpi=200); plt.close(fig)

